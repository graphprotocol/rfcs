<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Graph Protocol RFCs and Engineering Plans</title>
        
        <meta name="robots" content="noindex" />
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
        <link rel="stylesheet" href="assets/mermaid.css">
        
        <link rel="stylesheet" href="assets/custom.css">
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "light" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="rfcs/index.html"><strong aria-hidden="true">2.</strong> RFCs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rfcs/approved.html"><strong aria-hidden="true">2.1.</strong> Approved RFCs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rfcs/0001-subgraph-composition.html"><strong aria-hidden="true">2.1.1.</strong> RFC-0001: Subgraph Composition</a></li><li class="chapter-item expanded "><a href="rfcs/0002-ethereum-tracing-cache.html"><strong aria-hidden="true">2.1.2.</strong> RFC-0002: Ethereum Tracing Cache</a></li><li class="chapter-item expanded "><a href="rfcs/0003-mutations.html"><strong aria-hidden="true">2.1.3.</strong> RFC-0003: Mutations</a></li><li class="chapter-item expanded "><a href="rfcs/0004-fulltext-search.html"><strong aria-hidden="true">2.1.4.</strong> RFC-0004: Fulltext Search</a></li></ol></li><li class="chapter-item expanded "><a href="rfcs/obsolete.html"><strong aria-hidden="true">2.2.</strong> Obsolete RFCs</a></li><li class="chapter-item expanded "><a href="rfcs/rejected.html"><strong aria-hidden="true">2.3.</strong> Rejected RFCs</a></li></ol></li><li class="chapter-item expanded "><a href="engineering-plans/index.html"><strong aria-hidden="true">3.</strong> Engineering Plans</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="engineering-plans/approved.html"><strong aria-hidden="true">3.1.</strong> Approved Plans</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="engineering-plans/0001-graphql-query-prefetching.html"><strong aria-hidden="true">3.1.1.</strong> PLAN-0001: GraphQL Query Prefetching</a></li><li class="chapter-item expanded "><a href="engineering-plans/0002-ethereum-tracing-cache.html"><strong aria-hidden="true">3.1.2.</strong> PLAN-0002: Ethereum Tracing Cache</a></li><li class="chapter-item expanded "><a href="engineering-plans/0003-remove-jsonb-storage.html"><strong aria-hidden="true">3.1.3.</strong> PLAN-0003: Remove JSONB Storage</a></li></ol></li><li class="chapter-item expanded "><a href="engineering-plans/obsolete.html"><strong aria-hidden="true">3.2.</strong> Obsolete Plans</a></li><li class="chapter-item expanded "><a href="engineering-plans/rejected.html"><strong aria-hidden="true">3.3.</strong> Rejected Plans</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Graph Protocol RFCs and Engineering Plans</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>This repository / book describes the process for proposing changes to Graph
Protocol in the form of <a href="./rfcs/index.html">RFCs</a> and <a href="./engineering-plans/index.html">Engineering
Plans</a>.</p>
<p>It also includes all approved, rejected and obsolete RFCs and Engineering Plans.
For more details, see the following pages:</p>
<ul>
<li><a href="./rfcs/index.html">RFCs</a></li>
<li><a href="./engineering-plans/index.html">Engineering Plans</a></li>
</ul>
<h1><a class="header" href="#rfcs" id="rfcs">RFCs</a></h1>
<h2><a class="header" href="#what-is-an-rfc" id="what-is-an-rfc">What is an RFC?</a></h2>
<p>An RFC describes a change to Graph Protocol, for example a new feature. Any
substantial change goes through the RFC process, where the change is described
in an RFC, is proposed a pull request to the <code>rfcs</code> repository, is reviewed,
currently by the core team, and ultimately is either either approved or
rejected.</p>
<h2><a class="header" href="#rfc-process" id="rfc-process">RFC process</a></h2>
<h3><a class="header" href="#1-create-a-new-rfc" id="1-create-a-new-rfc">1. Create a new RFC</a></h3>
<p>RFCs are numbered, starting at <code>0001</code>. To create a new RFC, create a new branch
of the <code>rfcs</code> repository. Check the existing RFCs to identify the next number to
use. Then, copy the <a href="https://github.com/graphprotocol/rfcs/blob/master/rfcs/0000-template.md">RFC
template</a>
to a new file in the <code>rfcs/</code> directory. For example:</p>
<pre><code class="language-sh">cp rfcs/0000-template.md rfcs/0015-fulltext-search.md
</code></pre>
<p>Write the RFC, commit it to the branch and open a <a href="https://github.com/graphprotocol/rfcs/pulls">pull
request</a> in the <code>rfcs</code> repository.</p>
<p>In addition to the RFC itself, the pull request must include the following
changes:</p>
<ul>
<li>a link to the RFC on the <a href="rfcs/./approved.html">Approved RFCs</a> page, and</li>
<li>a link to the RFC under <code>Approved RFCs</code> in <code>SUMMARY.md</code>.</li>
</ul>
<h3><a class="header" href="#2-rfc-review" id="2-rfc-review">2. RFC review</a></h3>
<p>After an RFC has been submitted through a pull request, it is being reviewed. At
the time of writing, every RFC needs to be approved by</p>
<ul>
<li>at least one Graph Protocol founder, and</li>
<li>at least one member of the core development team.</li>
</ul>
<h3><a class="header" href="#3-rfc-approval" id="3-rfc-approval">3. RFC approval</a></h3>
<p>Once an RFC is approved, the RFC meta data (see the
<a href="https://github.com/graphprotocol/rfcs/blob/master/rfcs/0000-template.md">template</a>)
is updated and the pull request is merged by the original author or a Graph
Protocol team member.</p>
<h1><a class="header" href="#approved-rfcs" id="approved-rfcs">Approved RFCs</a></h1>
<ul>
<li><a href="rfcs/./0001-subgraph-composition.html">RFC-0001: Subgraph Composition</a></li>
<li><a href="rfcs/./0002-ethereum-tracing-cache.html">RFC-0002: Ethereum Tracing Cache</a></li>
<li><a href="rfcs/./0003-mutations.html">RFC-0003: Mutations</a></li>
<li><a href="rfcs/./rfcs/0004-fulltext-search.html">RFC-0004: Fulltext Search</a></li>
</ul>
<h1><a class="header" href="#rfc-0001-subgraph-composition" id="rfc-0001-subgraph-composition">RFC-0001: Subgraph Composition</a></h1>
<dl>
  <dt>Author</dt>
  <dd>Jannis Pohlmann</dd>
<dt>RFC pull request</dt>
  <dd><a href="https://github.com/graphprotocol/rfcs/pull/1">https://github.com/graphprotocol/rfcs/pull/1</a></dd>
<dt>Obsoletes</dt>
  <dd>-</dd>
<dt>Date of submission</dt>
  <dd>2019-12-08</dd>
<dt>Date of approval</dt>
  <dd>-</dd>
<dt>Approved by</dt>
  <dd>-</dd>
</dl>
<h2><a class="header" href="#summary" id="summary">Summary</a></h2>
<p>Subgraph composition enables referencing, extending and querying entities across
subgraph boundaries.</p>
<h2><a class="header" href="#goals--motivation" id="goals--motivation">Goals &amp; Motivation</a></h2>
<p>The high-level goal of subgraph composition is to be able to compose subgraph
schemas and data hierarchically. Imagine umbrella subgraphs that combine all the
data from a domain (e.g. DeFi, job markets, music) through one unified, coherent
API. This could allow reuse and governance at different levels and go all the
way to the top, fulfilling the vision of <em>the</em> Graph.</p>
<p>The ability to reference, extend and query entities across subgraph boundaries
enables several use cases:</p>
<ol>
<li>Linking entities across subgraphs.</li>
<li>Extending entities defined in other subgraphs by adding new fields.</li>
<li>Breaking down data silos by composing subgraphs and defining richer schemas
without indexing the same data over and over again.</li>
</ol>
<p>Subgraph composition is needed to avoid duplicated work, both in terms of
developing subgraphs as well as indexing them. It is an essential part of the
overall vision behind The Graph, as it allows to combine isolated subgraphs into
a complete, connected graph of the (decentralized) world's data.</p>
<p>Subgraph developers will benefit from the ability to reference data from other
subgraphs, saving them development time and enabling richer data models. dApp
developers will be able to leverage this to build more compelling applications.
Node operators will benefit from subgraph composition by having better insight
into which subgraphs are queried together, allowing them to make more informed
decisions about which subgraphs to index.</p>
<h2><a class="header" href="#urgency" id="urgency">Urgency</a></h2>
<p>Due to the high impact of this feature and its important role in fulfilling the
vision behind The Graph, it would be good to start working on this as early as
possible.</p>
<h2><a class="header" href="#terminology" id="terminology">Terminology</a></h2>
<p>The feature is referred to by <em>query-time subgraph composition</em>, short:
<em>subgraph composition</em>. </p>
<p>Terms introduced and used in this RFC:</p>
<ul>
<li><em>Imported schema</em>: The schema of another subgraph from which types are
imported.</li>
<li><em>Imported type</em>: An entity type imported from another subgraph schema.</li>
<li><em>Extended type</em>: An entity type imported from another subgraph schema and
extended in the subgraph that imports it.</li>
<li><em>Local schema</em>: The schema of the subgraph that imports from another subgraph.</li>
<li><em>Local type</em>: A type defined in the local schema.</li>
</ul>
<h2><a class="header" href="#detailed-design" id="detailed-design">Detailed Design</a></h2>
<p>The sections below make the assumption that there is a subgraph with the name
<code>ethereum/mainnet</code> that includes an <code>Address</code> entity type.</p>
<h3><a class="header" href="#composing-subgraphs-by-importing-types" id="composing-subgraphs-by-importing-types">Composing Subgraphs By Importing Types</a></h3>
<p>In order to reference entity types from annother subgraph, a developer would
first import these types from the other subgraph's schema.</p>
<p>Types can be imported either from a subgraph name or from a subgraph ID.
Importing from a subgraph name means that the exact version of the imported
subgraph will be identified at query time and its schema may change in arbitrary
ways over time. Importing from a subgraph ID guarantees that the schema will
never change but also means that the import points to a subgraph version that
may become outdated over time.</p>
<p>Let's say a DAO subgraph contains a <code>Proposal</code> type that has a <code>proposer</code> field
that should link to an Ethereum address (think: Ethereum accounts or contracts)
and a <code>transaction</code> field that should link to an Ethereum transaction. The
developer would then write the DAO subgraph schema as follows:</p>
<pre><code class="language-graphql">type _Schema_
  @import(
    types: [&quot;Address&quot;, { name: &quot;Transaction&quot;, as: &quot;EthereumTransaction&quot; }],
    from: { name: &quot;ethereum/mainnet&quot; }
  )

type Proposal @entity {
  id: ID!
  proposer: Address!
  transaction: EthereumTransaction!
}
</code></pre>
<p>This would then allow queries that follow the references to addresses and
transactions, like</p>
<pre><code class="language-graphql">{
  proposals { 
    proposer {
      balance
      address
    }
    transaction {
      hash
      block {
        number
      }
    }
  }
}
</code></pre>
<h3><a class="header" href="#extending-types-from-imported-schemas" id="extending-types-from-imported-schemas">Extending Types From Imported Schemas</a></h3>
<p>Extending types from another subgraph involves several steps:</p>
<ol>
<li>Importing the entity types from the other subgraph.</li>
<li>Extending these types with custom fields.</li>
<li>Managing (e.g. creating) extended entities in subgraph mappings.</li>
</ol>
<p>Let's say the DAO subgraph wants to extend the Ethereum <code>Address</code> type to
include the proposals created by each respective account. To achieve this, the
developer would write the following schema:</p>
<pre><code class="language-graphql">type _Schema_
  @import(
    types: [&quot;Address&quot;],
    from: { name: &quot;ethereum/mainnet&quot; }
  )

type Proposal @entity {
  id: ID!
  proposer: Address!
}

extend type Address {
  proposals: [Proposal!]! @derivedFrom(field: &quot;proposal&quot;)
}
</code></pre>
<p>This makes queries like the following possible, where the query can go &quot;back&quot;
from addresses to proposal entities, despite the Ethereum <code>Address</code> type
originally being defined in the <code>ethereum/mainnet</code> subgraph.</p>
<pre><code class="language-graphql">{
  addresses {
    id
    proposals {
      id
      proposer {
        id
    }
  }
}
</code></pre>
<p>In the above case, the <code>proposals</code> field on the extended type is derived, which
means that an implementation wouldn't have to create a local extension type in
the store. However, if <code>proposals</code> was defined as</p>
<pre><code class="language-graphql">extend type Address {
  proposals: [Proposal!]!
}
</code></pre>
<p>then it would the subgraph mappings would have to create partial <code>Address</code>
entities with <code>id</code> and <code>proposals</code> fields for all addresses from which proposals
were created. At query time, these entity instances would have to be merged with
the original <code>Address</code> entities from the <code>ethereum/mainnet</code> subgraph.</p>
<h3><a class="header" href="#subgraph-availability" id="subgraph-availability">Subgraph Availability</a></h3>
<p>In the decentralized network, queries will be split and routed through the
network based on what indexers are available and which subgraphs they index. At
that point, failure to find an indexer for a subgraph that types were imported
from will result in a query error. The error that a non-nullable field resolved
to null bubbles up to the next nullable parent, in accordance with the <a href="https://graphql.github.io/graphql-spec/draft/#sec-Errors.Error-result-format">GraphQL
Spec</a>.</p>
<p>Until the network is reality, we are dealing with individual Graph Nodes and
querying subgraphs where imported entity types are not also indexed on the same
node should be handled with more tolerance. This RFC proposes that entity
reference fields that refer to imported types are converted to being optional in
the generated API schema. If the subgraph that the type is imported from is not
available on a node, such fields should resolve to <code>null</code>.</p>
<h3><a class="header" href="#interfaces" id="interfaces">Interfaces</a></h3>
<p>Subgraph composition also supports interfaces in the ways outlined below.</p>
<h4><a class="header" href="#interfaces-can-be-imported-from-other-subgraphs" id="interfaces-can-be-imported-from-other-subgraphs">Interfaces Can Be Imported From Other Subgraphs</a></h4>
<p>The syntax for this is the same as that for importing types:</p>
<pre><code class="language-graphql">type _Schema_
  @import(types: [&quot;ERC20&quot;], from: { name: &quot;graphprotocol/erc20&quot; })
</code></pre>
<h4><a class="header" href="#local-types-can-implement-imported-interfaces" id="local-types-can-implement-imported-interfaces">Local Types Can Implement Imported Interfaces</a></h4>
<p>This is achieved by importing the interface from another subgraph schema
and implementing it in entity types:</p>
<pre><code class="language-graphql">type _Schema_
  @import(types: [&quot;ERC20&quot;], from: { name: &quot;graphprotocol/erc20&quot; })

type MyToken implements ERC20 @entity {
  # ...
}
</code></pre>
<h4><a class="header" href="#imported-types-can-be-extended-to-implement-local-interfaces" id="imported-types-can-be-extended-to-implement-local-interfaces">Imported Types Can Be Extended To Implement Local Interfaces</a></h4>
<p>This is achieved by importing the types from another subgraph schema, defining a
local interface and using <code>extend</code> to implement the interface on the imported
types:</p>
<pre><code class="language-graphql">type _Schema_
  @import(types: [{ name: &quot;Token&quot;, as &quot;LPT&quot; }], from: { name: &quot;livepeer/livepeer&quot; })
  @import(types: [{ name: &quot;Token&quot;, as &quot;Rep&quot; }], from: { name: &quot;augur/augur&quot; })

interface Token {
  id: ID!
  balance: BigInt!
}

extend LPT implements Token {
  # ...
}
extend Rep implements Token {
  # ...
}
</code></pre>
<h4><a class="header" href="#imported-types-can-be-extended-to-implement-imported-interfaces" id="imported-types-can-be-extended-to-implement-imported-interfaces">Imported Types Can Be Extended To Implement Imported Interfaces</a></h4>
<p>This is a combination of importing an interface, importing the types and
extending them to implement the interface:</p>
<pre><code class="language-graphql">type _Schema_
  @import(types: [&quot;Token&quot;], from: { name: &quot;graphprotocol/token&quot; })
  @import(types: [{ name: &quot;Token&quot;, as &quot;LPT&quot; }], from: { name: &quot;livepeer/livepeer&quot; })
  @import(types: [{ name: &quot;Token&quot;, as &quot;Rep&quot; }], from: { name: &quot;augur/augur&quot; })

extend LPT implements Token {
  # ...
}
extend Rep implements Token {
  # ...
}
</code></pre>
<h4><a class="header" href="#implementation-concerns-for-interface-support" id="implementation-concerns-for-interface-support">Implementation Concerns For Interface Support</a></h4>
<p>Querying across types from different subgraphs that implement the same interface
may require a smart algorithm, especially when it comes to pagination. For
instance, if the first 1000 entities for an interface are queried, this range of
1000 entities may be divided up between different local and imported types
arbitrarily.</p>
<p>A naive algorithm could request 1000 entities from each subgraph, applying the
selected filters and order, combine the results and cut off everything after the
first 1000 items. This would generate a minimum of requests but would involve
significant overfetching.</p>
<p>Another algorithm could just fetch the first item from each subgraph, then based
on that information, divide up the range in more optimal ways than the previous
algorith, and satisfy the query with more requests but with less overfetching.</p>
<h2><a class="header" href="#compatibility" id="compatibility">Compatibility</a></h2>
<p>Subgraph composition is a purely additive, non-breaking change. Existing
subgraphs remain valid without any migrations being necessary.</p>
<h2><a class="header" href="#drawbacks-and-risks" id="drawbacks-and-risks">Drawbacks And Risks</a></h2>
<p>Reasons that could speak against implementing this feature:</p>
<ul>
<li>
<p>Schema parsing and validation becomes more complicated. Especially validation
of imported schemas may not always be possible, depending on whether and when
the referenced subgraph is available on the Graph Node or not.</p>
</li>
<li>
<p>Query execution becomes more complicated. The subgraph a type belongs to must
be identified and local as well as imported versions of extended entities have
to be queried separately and be merged before returning data to the client.</p>
</li>
</ul>
<h2><a class="header" href="#alternatives" id="alternatives">Alternatives</a></h2>
<p>No alternatives have been considered.</p>
<p>There are other ways to compose subgraph schemas using GraphQL technologies such
as <a href="https://www.apollographql.com/docs/graphql-tools/schema-stitching/">schema
stitching</a>
or <a href="https://www.apollographql.com/docs/apollo-server/federation/introduction/">Apollo
Federation</a>.
However, schema stitching is being deprecated and Apollo Federation requires a
centralized server to serve to extend and merge GraphQL API. Both of these
solutions slow down queries.</p>
<p>Another reason not to use these is that GraphQL will only be <em>one</em> of several
query languages supported in the future. Composition therefore has to be
implemented in a query-language-agnostic way.</p>
<h2><a class="header" href="#open-questions" id="open-questions">Open Questions</a></h2>
<ul>
<li>
<p>Right now, interfaces require unique IDs across all the concrete entity types
that implement them. This is not something we can guarantee any longer if
these concrete types live in different subgraphs. So we have to handle this at
query time (or must somehow disallow it, returning a query error).</p>
<p>It is also unclear how an individual interface entity lookup would look like
if IDs are no longer guaranteed to be unique:</p>
<pre><code class="language-graphql">someInterface(id: &quot;?????&quot;) {
}
</code></pre>
</li>
</ul>
<h1><a class="header" href="#rfc-0002-ethereum-tracing-cache" id="rfc-0002-ethereum-tracing-cache">RFC-0002: Ethereum Tracing Cache</a></h1>
<dl>
  <dt>Author</dt>
  <dd>Zac Burns</dd>
<dt>RFC pull request</dt>
  <dd><a href="https://github.com/graphprotocol/rfcs/pull/4">https://github.com/graphprotocol/rfcs/pull/4</a></dd>
<dt>Obsoletes (if applicable)</dt>
  <dd>None</dd>
<dt>Date of submission</dt>
  <dd>2019-12-13</dd>
<dt>Date of approval</dt>
  <dd>2019-12-20</dd>
<dt>Approved by</dt>
  <dd>Jannis Pohlmann</dd>
</dl>
<h2><a class="header" href="#summary-1" id="summary-1">Summary</a></h2>
<p>This RFC proposes the creation of a local Ethereum tracing cache to speed up indexing of subgraphs which use block and/or call handlers.</p>
<h2><a class="header" href="#motivation" id="motivation">Motivation</a></h2>
<p>When indexing a subgraph that uses block and/or call handlers, it is necessary to extract calls from the trace of each block that a Graph Node indexes. It is expensive to acquire and process traces from Ethereum nodes in both money and time.</p>
<p>When developing a subgraph it is common to make changes and deploy those changes to a production Graph Node for testing. Each time a change is deployed, the Graph Node must re-sync the subgraph using the same traces that were used for the previous sync of the subgraph. The cost of acquiring the traces each time a change is deployed impacts a subgraph developer's ability to iterate and test quickly.</p>
<h2><a class="header" href="#urgency-1" id="urgency-1">Urgency</a></h2>
<p>None</p>
<h2><a class="header" href="#terminology-1" id="terminology-1">Terminology</a></h2>
<p><em>Ethereum cache</em>: The new API proposed here.</p>
<h2><a class="header" href="#detailed-design-1" id="detailed-design-1">Detailed Design</a></h2>
<p>There is an existing <code>EthereumCallCache</code> for caching <code>eth_call</code> built into Graph Node today. This cache will be extended to support traces, and renamed to <code>EthereumCache</code>.</p>
<h2><a class="header" href="#compatibility-1" id="compatibility-1">Compatibility</a></h2>
<p>This change is backwards compatible. Existing code can continue to use the parity tracing API. Because the cache is local, each indexing node may delete the cache should the format or implementation of caching change. In this case of invalidated cache the code will fall back to existing methods for retrieving a trace and repopulating the cache.</p>
<h2><a class="header" href="#drawbacks-and-risks-1" id="drawbacks-and-risks-1">Drawbacks and Risks</a></h2>
<p>Subgraphs which are not being actively developed will incur the overhead for storing traces, but will not ever reap the benefits of ever reading them back from the cache.</p>
<p>If this drawback is significant, it may be necessary to extend <code>EthereumCache</code> to provide a custom score for cache invalidation other than the current date. For example, <code>trace_filter</code> calls could be invalidated based on the latest update time for a subgraph requiring the trace. It is expected that a subgraph which has been updated recently is more likely to be updated again soon then a subgraph which has not been recently updated.</p>
<h2><a class="header" href="#alternatives-1" id="alternatives-1">Alternatives</a></h2>
<p>None</p>
<h2><a class="header" href="#open-questions-1" id="open-questions-1">Open Questions</a></h2>
<p>None</p>
<h1><a class="header" href="#rfc-0003-mutations" id="rfc-0003-mutations">RFC-0003: Mutations</a></h1>
<dl>
  <dt>Author</dt>
  <dd>dOrg: Jordan Ellis, Nestor Amesty</dd>
<dt>RFC pull request</dt>
  <dd><a href="https://github.com/graphprotocol/rfcs/pull/10">URL</a></dd>
<dt>Date of submission</dt>
  <dd>2019-12-20</dd>
<dt>Date of approval</dt>
  <dd>2020-2-03</dd>
<dt>Approved by</dt>
  <dd>Jannis Pohlmann</dd>
</dl>
<h2><a class="header" href="#contents" id="contents">Contents</a></h2>
<ul>
<li><a href="rfcs/0003-mutations.html#summary">Summary</a></li>
<li><a href="rfcs/0003-mutations.html#goals--motivation">Goals &amp; Motivation</a></li>
<li><a href="rfcs/0003-mutations.html#urgency">Urgency</a></li>
<li><a href="rfcs/0003-mutations.html#terminology">Terminology</a></li>
<li><a href="rfcs/0003-mutations.html#detailed-design">Detailed Design</a></li>
<li><a href="rfcs/0003-mutations.html#mutations-manifest">Mutations Manifest</a></li>
<li><a href="rfcs/0003-mutations.html#mutations-schema">Mutations Schema</a></li>
<li><a href="rfcs/0003-mutations.html#mutation-resolvers">Mutation Resolvers</a></li>
<li><a href="rfcs/0003-mutations.html#dapp-integration">dApp Integration</a></li>
<li><a href="rfcs/0003-mutations.html#compatibility">Compatibility</a></li>
<li><a href="rfcs/0003-mutations.html#drawbacks-and-risks">Drawbacks and Risks</a></li>
<li><a href="rfcs/0003-mutations.html#alternatives">Alternatives</a></li>
<li><a href="rfcs/0003-mutations.html#open-questions">Open Questions</a></li>
</ul>
<h2><a class="header" href="#summary-2" id="summary-2">Summary</a></h2>
<p>GraphQL mutations allow developers to add executable functions to their schema. Callers can invoke these functions using GraphQL queries. An introduction to how mutations are defined and work can be found <a href="https://graphql.org/learn/queries/#mutations">here</a>. This RFC will assume the reader understands how to use GraphQL mutations in a traditional Web2 application. This proposal describes how mutations are added to The Graph's toolchain, and used to replace Web3 write operations the same way The Graph has replaced Web3 read operations.</p>
<h2><a class="header" href="#goals--motivation-1" id="goals--motivation-1">Goals &amp; Motivation</a></h2>
<p>The Graph has created a read semantic layer that describes smart contract protocols, which has made it easier to build applications on top of complex protocols. Since dApps have two primary interactions with Web3 protocols (reading &amp; writing), the next logical addition is write support.</p>
<p>Protocol developers that use a subgraph still often publish a Javascript wrapper library for their dApp developers (examples: <a href="https://github.com/daostack/client">DAOstack</a>, <a href="https://github.com/ensdomains/ensjs">ENS</a>, <a href="https://github.com/livepeer/livepeerjs/tree/master/packages/sdk">LivePeer</a>, <a href="https://github.com/makerdao/dai.js/tree/dev/packages/dai">DAI</a>, <a href="https://github.com/Uniswap/uniswap-sdk">Uniswap</a>). This is done to help speed up dApp development and promote consistency with protocol usage patterns. With the addition of mutations to the Graph Protocol's GraphQL tooling, Web3 reading &amp; writing can now both be invoked through GraphQL queries. dApp developers can now simply refer to a single GraphQL schema that defines the entire protocol.</p>
<h2><a class="header" href="#urgency-2" id="urgency-2">Urgency</a></h2>
<p>This is urgent from a developer experience point of view. With this addition, it eliminates the need for protocol developers to manually wrap GraphQL query interfaces alongside developer-friendly write functions. Additionally, mutations provide a solution for optimistic UI updates, which is something dApp developers have been seeking for a long time (see <a href="https://github.com/aragon/nest/issues/21">here</a>). Lastly with the whole protocol now defined in GraphQL, existing application layer code generators can now be used to hasten dApp development (<a href="https://dev.to/graphqleditor/top-3-graphql-code-generators-1gnj">some examples</a>).</p>
<h2><a class="header" href="#terminology-2" id="terminology-2">Terminology</a></h2>
<ul>
<li><em>Mutations</em>: Collection of mutations.</li>
<li><em>Mutation</em>: A GraphQL mutation.</li>
<li><em>Mutations Schema</em>: A GraphQL schema that defines a <code>type Mutation</code>, which contains all mutations. Additionally this schema can define other types to be used by the mutations, such as <code>input</code> and <code>interface</code> types.</li>
<li><em>Mutations Manifest</em>: A YAML manifest file that is used to add mutations to an existing subgraph manifest. This manifest can be stored in an external YAML file, or within the subgraph manifest's YAML file under the <code>mutations</code> property.</li>
<li><em>Mutation Resolvers</em>: Code module that contains all resolvers.</li>
<li><em>Resolver</em>: Function that is used to execute a mutation's logic.</li>
<li><em>Mutation Context</em>: A context object that's created for every mutation that's executed. It's passed as the 3rd argument to the resolver function.</li>
<li><em>Mutation States</em>: A collection of mutation states. One is created for each mutation being executed in a given query.</li>
<li><em>Mutation State</em>: The state of a mutation being executed. Also referred to in this document as &quot;<em>State</em>&quot;. It is an aggregate of the core &amp; extended states (see below). dApp developers can subscribe to the mutation's state upon execution of the mutation query. See the <code>useMutation</code> examples below.</li>
<li><em>Core State</em>: Default properties present within every mutation state. Some examples: <code>events: Event[]</code>, <code>uuid: string</code>, and <code>progress: number</code>.</li>
<li><em>Extended State</em>: Properties the mutation developer defines. These are added alongside the core state properties in the mutation state. There are no bounds to what a developer can define here. See examples below.</li>
<li><em>State Events</em>: Events emitted by mutation resolvers. Also referred to in this document as &quot;<em>Events</em>&quot;. Events are defined by a <code>name: string</code> and a <code>payload: any</code>. These events, once emitted, are given to reducer functions which then update the state accordingly.</li>
<li><em>Core Events</em>: Default events available to all mutations. Some examples: <code>PROGRESS_UPDATE</code>, <code>TRANSACTION_CREATED</code>, <code>TRANSACTION_COMPLETED</code>.</li>
<li><em>Extended Events</em>: Events the mutation developer defines. See examples below.</li>
<li><em>State Reducers</em>: A collection of state reducer functions.</li>
<li><em>State Reducer</em>: Reducers are responsible for translating events into state updates. They take the form of a function that has the inputs [event, current state], and returns the new state post-event. Also referred to in this document as &quot;<em>Reducer(s)</em>&quot;.</li>
<li><em>Core Reducers</em>: Default reducers that handle the processing of the core events.</li>
<li><em>Extended Reducers</em>: Reducers the mutation developer defines. These reducers can be defined for any event, core or extended. The core &amp; extended reducers are run one after another if both are defined for a given core event. See examples below.</li>
<li><em>State Updater</em>: The state updater object is used by the resolvers to dispatch events. It's passed to the resolvers through the mutation context like so: <code>context.graph.state</code>.</li>
<li><em>State Builder</em>: An object responsible for (1) initializing the state with initial values and (2) defining reducers for events.</li>
<li><em>Core State Builder</em>: A state builder that's defined by default. It's responsible for initializing the core state properties, and processing the core events with its reducers.</li>
<li><em>Extended State Builder</em>: A state builder defined by the mutation developer. It's responsible for initializing the extended state properties, and processing the extended events with its reducers.</li>
<li><em>Mutations Config</em>: Collection of config properties required by the mutation resolvers. Also referred to in this document as &quot;<em>Config</em>&quot;. All resolvers share the same config. It's passed to the resolver through the mutation context like so: <code>context.graph.config</code>.</li>
<li><em>Config Property</em>: A single property within the config (ex: ipfs, ethereum, etc).</li>
<li><em>Config Generator</em>: A function that takes a config argument, and returns a config property. For example, &quot;localhost:5001&quot; as a config argument gets turned into a new IPFS client by the config generator.</li>
<li><em>Config Argument</em>: An initialization argument that's passed into the config generator function. This config argument is provided by the dApp developer.</li>
<li><em>Optimistic Response</em>: A response given to the dApp that predicts what the outcome of the mutation's execution will be. If it is incorrect, it will be overwritten with the actual result.</li>
</ul>
<h2><a class="header" href="#detailed-design-2" id="detailed-design-2">Detailed Design</a></h2>
<p>The sections below illustrate how a developer would add mutations to an existing subgraph, and then add those mutations to a dApp.</p>
<h3><a class="header" href="#mutations-manifest" id="mutations-manifest">Mutations Manifest</a></h3>
<p>The subgraph manifest (<code>subgraph.yaml</code>) now has an extra property named <code>mutations</code> which is the mutations manifest.</p>
<p><code>subgraph.yaml</code></p>
<pre><code class="language-yaml">specVersion: ...
...
mutations:
  repository: https://npmjs.com/package/...
  schema:
    file: ./mutations/schema.graphql
  resolvers:
    apiVersion: 0.0.1
    kind: javascript/es5
    file: ./mutations/index.js
    types: ./mutations/index.d.ts
dataSources: ...
...
</code></pre>
<p>Alternatively, the mutation manifest can be external like so:<br />
<code>subgraph.yaml</code></p>
<pre><code class="language-yaml">specVersion: ...
...
mutations:
  file: ./mutations/mutations.yaml
dataSources: ...
...
</code></pre>
<p><code>mutations/mutations.yaml</code></p>
<pre><code class="language-yaml">specVersion: ...
repository: https://npmjs.com/package/...
schema:
  file: ./schema.graphql
resolvers:
  apiVersion: 0.0.1
  kind: javascript/es5
  file: ./index.js
  types: ./index.d.ts
</code></pre>
<p>NOTE: <code>resolvers.types</code> is required. More on this below.</p>
<h3><a class="header" href="#mutations-schema" id="mutations-schema">Mutations Schema</a></h3>
<p>The mutations schema defines all of the mutations in the subgraph. The mutations schema builds on the subgraph schema, allowing the use of types from the subgraph schema, as well as defining new types that are used only in the context of mutations. For example, starting from a base subgraph schema:<br />
<code>schema.graphql</code></p>
<pre><code class="language-graphql">type MyEntity @entity {
  id: ID!
  name: String!
  value: BigInt!
}
</code></pre>
<p>Developers can define mutations that reference these subgraph schema types. Additionally new <code>input</code> and <code>interface</code> types can be defined for the mutations to use:<br />
<code>mutations/schema.graphql</code></p>
<pre><code class="language-graphql">input MyEntityOptions {
  name: String!
  value: BigInt!
}

interface NewNameSet {
  oldName: String!
  newName: String!
}

type Mutation {
  createEntity(
    options: MyEntityOptions!
  ): MyEntity!

  setEntityName(
    entity: MyEntity!
    name: String!
  ): NewNameSet!
}
</code></pre>
<p><code>graph-cli</code> handles the parsing and validating of these two schemas. It verifies that the mutations schema defines a <code>type Mutation</code> and that all of the mutations within it are defined in the resolvers module (see next section).</p>
<h3><a class="header" href="#mutation-resolvers" id="mutation-resolvers">Mutation Resolvers</a></h3>
<p>Each mutation within the schema must have a corresponding resolver function defined. Resolvers will be invoked by whatever engine executes the mutation queries (ex: Apollo Client). They are executed locally within the client application.</p>
<p>Mutation resolvers of kind <code>javascript/es5</code> take the form of an ES5 javascript module. This module is expected to have a default export that contains the following properties:</p>
<ul>
<li>
<p><code>resolvers: MutationResolvers</code> - The mutation resolver functions. The shape of this object must match the shape of the <code>type Mutation</code> defined above. See the example below for demonstration of this. Resolvers have the following prototype, <a href="https://github.com/graphql/graphql-js/blob/9dba58eeb6e28031bec7594b6df34c4fd74459b0/src/type/definition.js#L906">as defined in graphql-js</a>:</p>
<pre><code class="language-typescript">import { GraphQLFieldResolver } from 'graphql'

interface MutationContext&lt;
  TConfig extends ConfigGenerators,
  TState,
  TEventMap extends EventTypeMap
&gt; {
  [prop: string]: any,
  graph: {
    config: ConfigProperties&lt;TConfig&gt;,
    dataSources: DataSources,
    state: StateUpdater&lt;TState, TEventMap&gt;
  }
}

interface MutationResolvers&lt;
  TConfig extends ConfigGenerators,
  TState,
  TEventMap extends EventTypeMap
&gt; {
  Mutation: {
      [field: string]: GraphQLFieldResolver&lt;
        any,
        MutationContext&lt;TConfig, TState, TEventMap&gt;
      &gt;
  }
}
</code></pre>
</li>
<li>
<p><code>config: ConfigGenerators</code> - A collection of config generators. The config object is made up of properties, that can be nested, but all terminate in the form of a function with the prototype:</p>
<pre><code class="language-typescript">type ConfigGenerator&lt;TArg, TRet&gt; = (arg: TArg) =&gt; TRet

interface ConfigGenerators {
  [prop: string]: ConfigGenerator&lt;any, any&gt; | ConfigGenerators
}
</code></pre>
<p>See the example below for a demonstration of this.</p>
</li>
<li>
<p><code>stateBuilder: StateBuilder</code> (optional) - A state builder interface responsible for (1) initializing extended state properties and (2) reducing extended state events. State builders implement the following interface:</p>
<pre><code class="language-typescript">type MutationState&lt;TState&gt; = CoreState &amp; TState
type MutationEvents&lt;TEventMap&gt; = CoreEvents &amp; TEventMap

interface StateBuilder&lt;TState, TEventMap extends EventTypeMap&gt; {
  getInitialState(uuid: string): TState,
  // Event Specific Reducers
  reducers?: {
    [TEvent in keyof MutationEvents&lt;TEventMap&gt;]?: (
      state: MutationState&lt;TState&gt;,
      payload: InferEventPayload&lt;TEvent, TEventMap&gt;
    ) =&gt; OptionalAsync&lt;Partial&lt;MutationState&lt;TState&gt;&gt;&gt;
  },
  // Catch-All Reducer
  reducer?: (
    state: MutationState&lt;TState&gt;,
    event: Event
  ) =&gt; OptionalAsync&lt;Partial&lt;MutationState&lt;TState&gt;&gt;&gt;
}

interface EventPayload { }

interface Event {
  name: string
  payload: EventPayload
}

interface EventTypeMap {
  [name: string]: EventPayload
}

// Optionally support async functions
type OptionalAsync&lt;T&gt; = Promise&lt;T&gt; | T

// Infer the payload type from the event name, given an EventTypeMap
type InferEventPayload&lt;
  TEvent extends keyof TEvents,
  TEvents extends EventTypeMap
&gt; = TEvent extends keyof TEvents ? TEvents[TEvent] : any
</code></pre>
<p>See the example below for a demonstration of this.</p>
</li>
</ul>
<p>For example:<br />
<code>mutations/index.js</code></p>
<pre><code class="language-typescript">import {
  Event,
  EventPayload,
  MutationContext,
  MutationResolvers,
  MutationState,
  StateBuilder,
  ProgressUpdateEvent
} from &quot;@graphprotocol/mutations&quot;

import gql from &quot;graphql-tag&quot;
import { ethers } from &quot;ethers&quot;
import {
  AsyncSendable,
  Web3Provider
} from &quot;ethers/providers&quot;
import IPFS from &quot;ipfs&quot;

// Typesafe Context
type Context = MutationContext&lt;Config, State, EventMap&gt;

/// Mutation Resolvers
const resolvers: MutationResolvers&lt;Config, State, EventMap&gt; = {
  Mutation: {
    async createEntity (source: any, args: any, context: Context) {
      // Extract mutation arguments
      const { name, value } = args.options

      // Use config properties created by the
      // config generator functions
      const { ethereum, ipfs } = context.graph.config

      // Create ethereum transactions...
      // Fetch &amp; upload to ipfs...

      // Dispatch a state event through the state updater
      const { state } = context.graph
      await state.dispatch(&quot;PROGRESS_UPDATE&quot;, { progress: 0.5 })

      // Dispatch a custom extended event
      await state.dispatch(&quot;MY_EVENT&quot;, { myValue: &quot;...&quot; })

      // Get a copy of the current state
      const currentState = state.current

      // Send another query using the same client.
      // This query would result in the graph-node's
      // entity store being fetched from. You could also
      // execute another mutation here if desired.
      const { client } = context
      await client.query({
        query: gql`
          myEntity (id: &quot;${id}&quot;) {
            id
            name
            value
          }
        }`
      })

      ...
    },
    async setEntityName (source: any, args: any, context: Context) {
      ...
    }
  }
}

/// Config Generators
type Config = typeof config

const config = {
  // These function arguments are passed in by the dApp
  ethereum: (arg: AsyncSendable): Web3Provider =&gt; {
    return new ethers.providers.Web3Provider(arg)
  },
  ipfs: (arg: string): IPFS =&gt; {
    return new IPFS(arg)
  },
  // Example of a custom config property
  property: {
    // Generators can be nested
    a: (arg: string) =&gt; { },
    b: (arg: string) =&gt; { }
  }
}

/// (optional) Extended State, Events, and State Builder

// Extended State
interface State {
  myValue: string
}

// Extended Events
interface MyEvent extends EventPayload {
  myValue: string
}

type EventMap = {
  &quot;MY_EVENT&quot;: MyEvent
}

// Extended State Builder
const stateBuilder: StateBuilder&lt;State, EventMap&gt; = {
  getInitialState(): State {
    return {
      myValue: &quot;&quot;
    }
  },
  reducers: {
    &quot;MY_EVENT&quot;: async (state: MutationState&lt;State&gt;, payload: MyEvent) =&gt; {
      return {
        myValue: payload.myValue
      }
    },
    &quot;PROGRESS_UPDATE&quot;: (state: MutationState&lt;State&gt;, payload: ProgressUpdateEvent) =&gt; {
      // Do something custom...
    }
  },
  // Catch-all reducer...
  reducer: (state: MutationState&lt;State&gt;, event: Event) =&gt; {
    switch (event.name) {
      case &quot;TRANSACTION_CREATED&quot;:
        // Do something custom...
        break
    }
  }
}

export default {
  resolvers,
  config,
  stateBuilder
}

// Required Types
export {
  Config,
  State,
  EventMap,
  MyEvent
}
</code></pre>
<p>NOTE: It's expected that the mutations manifest has a <code>resolvers.types</code> file defined. The following types must be defined in the .d.ts type definition file:</p>
<ul>
<li><code>Config</code></li>
<li><code>State</code></li>
<li><code>EventMap</code></li>
<li>Any <code>EventPayload</code> interfaces defined within the <code>EventMap</code></li>
</ul>
<h3><a class="header" href="#dapp-integration" id="dapp-integration">dApp Integration</a></h3>
<p>In addition to the resolvers module defined above, the dApp has access to a run-time API to help with the instantiation and execution of mutations. This package is called <code>@graphprotocol/mutations</code> and is defined like so:</p>
<ul>
<li>
<p><code>createMutations</code> - Create a mutations interface which enables the user to <code>execute</code> a mutation query and <code>configure</code> the mutation module.</p>
<pre><code class="language-typescript">interface CreateMutationsOptions&lt;
  TConfig extends ConfigGenerators,
  TState,
  TEventMap extends EventTypeMap
&gt; {
  mutations: MutationsModule&lt;TConfig, TState, TEventMap&gt;,
  subgraph: string,
  node: string,
  config: ConfigArguments&lt;TConfig&gt;
  mutationExecutor?: MutationExecutor&lt;TConfig, TState, TEventMap&gt;
}

interface Mutations&lt;
  TConfig extends ConfigGenerators,
  TState,
  TEventMap extends EventTypeMap
&gt; {
  execute: (query: MutationQuery&lt;TConfig, TState, TEventMap&gt;) =&gt; Promise&lt;MutationResult&gt;
  configure: (config: ConfigArguments&lt;TConfig&gt;) =&gt; void
}

const createMutations = &lt;
  TConfig extends ConfigGenerators,
  TState = CoreState,
  TEventMap extends EventTypeMap = { },
&gt;(
  options: CreateMutationsOptions&lt;TConfig, TState, TEventMap&gt;
): Mutations&lt;TConfig, TState, TEventMap&gt; =&gt; { ... }
</code></pre>
</li>
<li>
<p><code>createMutationsLink</code> - wrap the mutations created above in an ApolloLink.</p>
<pre><code class="language-typescript">const createMutationsLink = &lt;
  TConfig extends ConfigGenerators,
  TState,
  TEventMap extends EventTypeMap,
&gt; (
  { mutations }: { mutations: Mutations&lt;TConfig, TState, TEventMap&gt; }
): ApolloLink =&gt; { ... }
</code></pre>
</li>
</ul>
<p>For applications using Apollo and React, a run-time API is available which mimics commonly used hooks and components for executing mutations, with the addition of having the mutation state available to the caller. This package is called <code>@graphprotocol/mutations-apollo-react</code> and is defined like so:</p>
<ul>
<li>
<p><code>useMutation</code> - see https://www.apollographql.com/docs/react/data/mutations/#executing-a-mutation</p>
<pre><code class="language-typescript">import { DocumentNode } from &quot;graphql&quot;
import {
  ExecutionResult,
  MutationFunctionOptions,
  MutationResult,
  OperationVariables
} from &quot;@apollo/react-common&quot;
import { MutationHookOptions } from &quot;@apollo/react-hooks&quot;
import { CoreState } from &quot;@graphprotocol/mutations&quot;

type MutationStates&lt;TState&gt; = {
  [mutation: string]: MutationState&lt;TState&gt;
}

interface MutationResultWithState&lt;TState, TData = any&gt; extends MutationResult&lt;TData&gt; {
  state: MutationStates&lt;TState&gt;
}

type MutationTupleWithState&lt;TState, TData, TVariables&gt; = [
  (
    options?: MutationFunctionOptions&lt;TData, TVariables&gt;
  ) =&gt; Promise&lt;ExecutionResult&lt;TData&gt;&gt;,
  MutationResultWithState&lt;TState, TData&gt;
]

const useMutation = &lt;
  TState = CoreState,
  TData = any,
  TVariables = OperationVariables
&gt;(
  mutation: DocumentNode,
  mutationOptions: MutationHookOptions&lt;TData, TVariables&gt;
): MutationTupleWithState&lt;TState, TData, TVariables&gt; =&gt; { ... }
</code></pre>
</li>
<li>
<p><code>Mutation</code> - see https://www.howtographql.com/react-apollo/3-mutations-creating-links/</p>
<pre><code class="language-typescript">interface MutationComponentOptionsWithState&lt;
  TState,
  TData,
  TVariables
&gt; extends BaseMutationOptions&lt;TData, TVariables&gt; {
  mutation: DocumentNode
  children: (
    mutateFunction: MutationFunction&lt;TData, TVariables&gt;,
    result: MutationResultWithState&lt;TState, TData&gt;
  ) =&gt; JSX.Element | null
}

const Mutation = &lt;
  TState = CoreState,
  TData = any,
  TVariables = OperationVariables
&gt;(
  props: MutationComponentOptionsWithState&lt;TState, TData, TVariables&gt;
): JSX.Element | null =&gt; { ... }
</code></pre>
</li>
</ul>
<p>For example:<br />
<code>dApp/src/App.tsx</code></p>
<pre><code class="language-typescript">import {
  createMutations,
  createMutationsLink
} from &quot;@graphprotocol/mutations&quot;
import {
  Mutation,
  useMutation
} from &quot;@graphprotocol/mutations-apollo-react&quot;
import myMutations, { State } from &quot;mutations-js-module&quot;
import { createHttpLink } from &quot;apollo-link-http&quot;

const mutations = createMutations({
  mutations: myMutations,
  // Config args, which will be passed to the generators
  config: {
    // Config args can take the form of functions to allow
    // for dynamic fetching behavior
    ethereum: async (): AsyncSendable =&gt; {
      const { ethereum } = (window as any)
      await ethereum.enable()
      return ethereum
    },
    ipfs: &quot;http://localhost:5001&quot;,
    property: {
      a: &quot;...&quot;,
      b: &quot;...&quot;
    }
  },
  subgraph: &quot;my-subgraph&quot;,
  node: &quot;http://localhost:8080&quot;
})

// Create Apollo links to handle queries and mutation queries
const mutationLink = createMutationLink({ mutations })
const queryLink = createHttpLink({
  uri: &quot;http://localhost:8080/subgraphs/name/my-subgraph&quot;
})

// Create a root ApolloLink which splits queries between
// the two different operation links (query &amp; mutation)
const link = split(
  ({ query }) =&gt; {
    const node = getMainDefinition(query)
    return node.kind === &quot;OperationDefinition&quot; &amp;&amp;
           node.operation === &quot;mutation&quot;
  },
  mutationLink,
  queryLink
)

// Create an Apollo Client
const client = new ApolloClient({
  link,
  cache: new InMemoryCache()
})

const CREATE_ENTITY = gql`
  mutation createEntity($options: MyEntityOptions) {
    createEntity(options: $options) {
      id
      name
      value
    }
  }
`

// exec: execution function for the mutation query
// loading: https://www.apollographql.com/docs/react/data/mutations/#tracking-mutation-status
// state: mutation state instance
const [exec, { loading, state }] = useMutation&lt;State&gt;(
  CREATE_ENTITY,
  {
    client,
    variables: {
      options: { name: &quot;...&quot;, value: 5 }
    }
  }
)

// Access the mutation's state like so:
state.createEntity.myValue

// Optimistic responses can be used to update
// the UI before the execution has finished.
// More information can be found here:
// https://www.apollographql.com/docs/react/performance/optimistic-ui/
const [exec, { loading, state }] = useMutation(
  CREATE_ENTITY,
  {
    optimisticResponse: {
      __typename: &quot;Mutation&quot;,
      createEntity: {
        __typename: &quot;MyEntity&quot;,
        name: &quot;...&quot;,
        value: 5,
        // NOTE: ID must be known so the
        // final response can be correlated.
        // Please refer to Apollo's docs.
        id: &quot;id&quot;
      }
    },
    variables: {
      options: { name: &quot;...&quot;, value: 5 }
    }
  }
)
</code></pre>
<pre><code class="language-html">// Use the Mutation JSX Component
&lt;Mutation
  mutation={CREATE_ENTITY}
  variables={{options: { name: &quot;...&quot;, value: 5 }}}
&gt;
  {(exec, { loading, state }) =&gt; (
    &lt;button onClick={exec} /&gt;
  )}
&lt;/Mutation&gt;
</code></pre>
<h2><a class="header" href="#compatibility-2" id="compatibility-2">Compatibility</a></h2>
<p>No breaking changes will be introduced, as mutations are an optional add-on to a subgraph.</p>
<h2><a class="header" href="#drawbacks-and-risks-2" id="drawbacks-and-risks-2">Drawbacks and Risks</a></h2>
<p>Nothing apparent at the moment.</p>
<h2><a class="header" href="#alternatives-2" id="alternatives-2">Alternatives</a></h2>
<p>The existing alternative that protocol developers are creating for dApp developers has been described above.</p>
<h2><a class="header" href="#open-questions-2" id="open-questions-2">Open Questions</a></h2>
<ul>
<li>
<p><strong>How can mutations pickup where they left off in the event of an abrupt application shutdown?</strong>
Since mutations can contain many different steps internally, it would be ideal to be able to support continuing resolver execution in the event the dApp abruptly shuts down.</p>
</li>
<li>
<p><strong>How can dApps understand what steps a given mutation will take during the course of its execution?</strong>
dApps may want to present to the user friendly progress updates, letting them know a given mutation is 3/4ths of the way through its execution (for example) and a high level description of each step. I view this as closely tied to the previous open question above, as we could support continuing resolver executions if we know what step it's currently undergoing. A potential implementation could include adding a <code>steps: Step[]</code> property to the core state, where <code>Step</code> looks similar to:</p>
<pre><code class="language-typescript">interface Step {
  id: string
  title: string
  description: string
  status: 'pending' | 'processing' | 'error' | 'finished'
  current: boolean
  error?: Error
  data: any
}
</code></pre>
<p>This, plus a few core events &amp; reducers, would be all we need to render UIs like the ones seen here: https://ant.design/components/steps/</p>
</li>
<li>
<p><strong>Should dApps be able to define event handlers for mutation events?</strong>
dApps may want to implement their own handlers for specific events emitted from mutations. These handlers would be different from the reducers, as we wouldn't want them to be able to modify the state. Instead they could store their own state elsewhere within the dApp based on the events.</p>
</li>
<li>
<p><strong>Should the Graph Node's schema introspection endpoint respond with the &quot;full&quot; schema, including the mutations' schema?</strong>
Developers could fetch the &quot;full&quot; schema by looking up the subgraph's manifest, read the <code>mutations.schema.file</code> hash value, and fetching the full schema from IPFS. Should the graph-node support querying this full schema directly from the graph-node itself through the introspection endpoint?</p>
</li>
<li>
<p><strong>Will server side execution ever be a reality?</strong>
I have not thought of a trustless solution to this, am curious if anyone has any ideas of how we could make this possible.</p>
</li>
<li>
<p><strong>Will The Graph Explorer support mutations?</strong>
We could have the explorer client-side application dynamically fetch and include mutation resolver modules. Configuring the resolvers module dynamically is problematic though. Maybe there are a few known config properties that the explorer client supports, and for all others it allows the user to input config arguments (if they're base types).</p>
</li>
</ul>
<h1><a class="header" href="#rfc-0004-fulltext-search" id="rfc-0004-fulltext-search">RFC-0004: Fulltext Search</a></h1>
<dl>
  <dt>Author</dt>
  <dd>Ford Nickels</dd>
<dt>RFC pull request</dt>
  <dd><a href="https://github.com/graphprotocol/rfcs/pull/11">URL</a></dd>
<dt>Obsoletes (if applicable)</dt>
  <dd>-</dd>
<dt>Date of submission</dt>
  <dd>2020-01-05</dd>
<dt>Date of approval</dt>
  <dd>2020-02-10</dd>
<dt>Approved by</dt>
  <dd>Jannis Pohlmann</dd>
</dl>
<h2><a class="header" href="#contents-1" id="contents-1">Contents</a></h2>
<ul>
<li><a href="rfcs/0004-fulltext-search.html#summary">Summary</a></li>
<li><a href="rfcs/0004-fulltext-search.html#goals--motivation">Goals &amp; Motivation</a></li>
<li><a href="rfcs/0004-fulltext-search.html#urgency">Urgency</a></li>
<li><a href="rfcs/0004-fulltext-search.html#terminology">Terminology</a></li>
<li><a href="rfcs/0004-fulltext-search.html#detailed-design">Detailed Design</a></li>
<li><a href="rfcs/0004-fulltext-search.html#subgraph-schema">Subgraph Schema</a></li>
<li><a href="rfcs/0004-fulltext-search.html#graphql-query-interface">GraphQL Query interface</a></li>
<li><a href="rfcs/0004-fulltext-search.html#tools-and-design">Tools and Design</a></li>
<li><a href="rfcs/0004-fulltext-search.html#compatibility">Compatibility</a></li>
<li><a href="rfcs/0004-fulltext-search.html#drawbacks-and-risks">Drawbacks and Risks</a></li>
<li><a href="rfcs/0004-fulltext-search.html#alternatives">Alternatives</a></li>
<li><a href="rfcs/0004-fulltext-search.html#open-questions">Open Questions</a></li>
</ul>
<h2><a class="header" href="#summary-3" id="summary-3">Summary</a></h2>
<p>The fulltext search filter type is a feature of the GraphQL API that
allows subgraph developers to specify language-specific, lexical,
composite filters that end users can use in their queries. The fulltext
search feature examines all words in a document, breaking it into
individual words and phrases (lexical analysis), and collapsing
variations of words into a single index term (stemming.)</p>
<h2><a class="header" href="#goals--motivation-2" id="goals--motivation-2">Goals &amp; Motivation</a></h2>
<p>The current set of string filters available in the GraphQL API is lacking 
fulltext search capabilities that enable efficient searches across entities
and attributes. Wildcard string matching does provide string filtering, but 
users have come to expect the easy to use filtering that comes with fulltext 
search systems.</p>
<p>To facilitate building effective user interfaces human-user friendly query 
filtering is essential. Lexical, composite fulltext search filters can provide 
the tools necessary for front-end developers to implement powerful search 
bars that filter data across multiple fields of an Entity.</p>
<p>The proposed feature aims to provide tools for subgraph developers to define 
composite search APIs that can search across multiple fields and entities.</p>
<h2><a class="header" href="#urgency-3" id="urgency-3">Urgency</a></h2>
<p>A delay in adding the fulltext search feature will not create issues
with current deployments. However, the feature will represent a
realization of part of the long term vision for the query network. In
addition, several high profile users have communicated that it may be a
conversion blocker. Implementation should be prioritized. </p>
<h2><a class="header" href="#terminology-3" id="terminology-3">Terminology</a></h2>
<ul>
<li>
<p><em>lexeme</em>: a basic lexical unit of a language, consisting of one word or 
several words, considered as an abstract unit, and applied to a family 
of words related by form or meaning.</p>
</li>
<li>
<p><em>morphology (linguistics)</em>: the study of words, how they are formed, 
and their relationship to other words in the same language. </p>
</li>
<li>
<p><em>fulltext search index</em>: the result of lexical and morphological 
analysis (stemming) of a set of text documents.  It provides frequency 
and location for the language-specific stems found in the text documents 
being indexed. </p>
</li>
<li>
<p><em>ranking algorithm</em>: &quot;Ranking attempts to measure how relevant documents 
are to a particular query, so that when there are many matches the most 
relevant ones can be shown first.&quot; <a href="https://www.postgresql.org/docs/11/textsearch-controls.html#TEXTSEARCH-RANKING-SEARCH-RESULTS">- Postgres Documentation</a></p>
<p><strong>Algorithms</strong>:</p>
<ul>
<li><em>standard ranking</em>: ranking based on the number of matching lexemes.</li>
<li><em>cover density ranking</em>: Cover density is similar to the standard 
fulltext search ranking except that the proximity of matching lexemes 
to each other is taken into consideration. This function requires 
lexeme positional information to perform its calculation, so it ignores 
any &quot;stripped&quot; lexemes in the index.</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#detailed-design-3" id="detailed-design-3">Detailed Design</a></h2>
<h3><a class="header" href="#subgraph-schema" id="subgraph-schema">Subgraph Schema</a></h3>
<p>Part of the power of the fulltext search API is the flexibility, so 
it is important to expose a simple interface to facilitate useful applications 
of the index and aim to reduce the need to create new subgraphs for the 
express purpose of updating fulltext search fields. </p>
<p>For each fulltext search API a subgraph developer must be able to specify:</p>
<ol>
<li>a language (specified using an <code>ISO 639-1</code> code), </li>
<li>a set of text document fields to include,</li>
<li>relative weighting for each field,</li>
<li>a choice of ranking algorithm for sorting query result items.</li>
</ol>
<p>The proposed process of adding one or more fulltext search API involves
adding one or more fulltext directive to the <code>_Schema_</code> type in the
subgraph's GraphQL schema. Each fulltext definition will have four
required top level parameters: <code>name</code>, <code>language</code>, <code>algorithm</code>, and
<code>include</code>. The fulltext search definitions will be used to generate
query fields on the GraphQL schema that will be exposed to the end user.</p>
<p>Enabling fulltext search across entities will be a powerful abstraction 
that allows users to search across all relevant entities in one query. Such 
a search will by definition have polymorphic results. To address this, a 
union type will be generated in the schema for the fulltext search results. </p>
<p>Validation of the fulltext definition will ensure that all fields referenced 
in the directive are valid String type fields. With subgraph composition 
it will be possible to easily create new subgraphs that add specific fulltext 
search capabilities to an existing subgraph. </p>
<p>Example fulltext search definition:</p>
<pre><code class="language-graphql">type _Schema_ 
  @fulltext(
    name: &quot;media&quot;
    ...
  )
  @fulltext(
    name: &quot;search&quot;,
    language: EN, # variant of `_FullTextLanguage` enum
    algorithm: RANKED, # variant of `_FullTextAlgorithm` enum
    include: [
      {
        entity: &quot;Band&quot;,
        fields: [
          { name: &quot;name&quot;, weight: 5 },
        ]
      },
      {
        entity: &quot;Album&quot;,
        fields: [
          { name: &quot;title&quot;, weight: 5 },
        ]
      },
      {
        entity: &quot;Musician&quot;,
        fields: [
          { name: &quot;name&quot;, weight: 10 },
          { name: &quot;bio&quot;, weight: 5 },
        ]
      }
    ]
  )
</code></pre>
<p>The schema generated from the above definition: </p>
<pre><code class="language-graphql">union _FulltextMediaEntity = ...
union _FulltextSearchEntity = Band | Album | Musician
type Query {
  media...
  search(text: String!, first: Int, skip: Int, block: Block_height): [FulltextSearchResultItem!]!
}
</code></pre>
<h3><a class="header" href="#graphql-query-interface" id="graphql-query-interface">GraphQL Query interface</a></h3>
<p>End users of the subgraph will have access to the fulltext search
queries alongside the other queries available for each entity in the
subgraph. In the case of a fulltext search defined across multiple
entities,
<a href="https://graphql.org/learn/queries/#inline-fragments">inline fragments</a>
may be used in the query to deal with the polymorphic result items. In
the front-end the <code>__typename</code> field can be used to distinguish the
concrete entity types of the returned results.</p>
<p>In the <code>text</code> parameter supplied to the query there will be several operators
available to the end user.  Included are the and, or, and proximity operators
(<code>&amp;</code>, <code>|</code>, <code>&lt;-&gt;</code>.) The special, proximity operator allows clients to specify 
the maximum distance between search terms: <code>foo&lt;3&gt;bar</code> is equivalent to 
requesting that <code>foo</code> and <code>bar</code> are at most three words apart. </p>
<p>Example query using inline fragments and the proximity operator: </p>
<pre><code class="language-graphql">query {
  search(text: &quot;Bob&lt;3&gt;run&quot;) {
    __typename
    ... on Band { name label { id } }
    ... on Album { title numberOfTracks }
    ... on Musician { name bio }
  }
}
</code></pre>
<h3><a class="header" href="#tools-and-design" id="tools-and-design">Tools and Design</a></h3>
<p>Fulltext search query system implementations often involve specific systems 
for storing and querying the text documents; however, in an effort to reduce 
system complexity and feature implementation time I propose starting with 
extending the current store interface and storage implemenation with fulltext 
search features rather than use a fulltext specific interface and storage
system.</p>
<p>A FullText search field will get its own column in a table dedicated to fulltext
data. The data stored will be the result of the lexical, morphological analysis 
of text documents performed on the fields included in the index. The fulltext 
search field will be created using the Postgres ts_vector function and will 
be indexed using a GIN index. The subgraph developer will define a ranking 
algorithm to be used to sort query results,so the end-user facing API remains 
easy to use without any requirement to understand the ranking algorithms.</p>
<h2><a class="header" href="#compatibility-3" id="compatibility-3">Compatibility</a></h2>
<p>This proposal does not change any existing interfaces, so no migrations 
will be necessary for existing subgraph deployments. </p>
<h2><a class="header" href="#drawbacks-and-risks-3" id="drawbacks-and-risks-3">Drawbacks and Risks</a></h2>
<p>The proposed solution uses native Postgres fulltext features and there is 
a nonzero probability this choice results in slower than optimal write and 
read times; however the tradeoff in implementation time/complexity and the 
existence of production use case testimonials tempers my apprehension here. </p>
<p>In future phases of the network the storage layer may get a redesign with 
indexes being overhauled to facilitate query result verification. Postgres
based fulltext search implementation would not be translatable to another 
storage system, so at the least a reevaluation of the tools used for analysis, 
indexing, and querying would be required.</p>
<h2><a class="header" href="#alternatives-3" id="alternatives-3">Alternatives</a></h2>
<p>An alternative design for the feature would allow more flexibility for
Graph Node operators in their index implementation and create a
marketplace for indexes. In the alternate, the definition of fulltext
search indexes could be moved out of the subgraph schema. The subgraph
would be deployed without them and they could be added later using a new
Graph Explorer interface (in Hosted-Service context) or a JSON-RPC
request directly to a Graph Node. Moving the creation of fulltext search
indexes/queries out of the schema would mean that that the definition of
uniqueness for a subgraph does not include the custom indexes, so a new
subgraph deployment and subgraph re-syncing work does not have to be
added in order to create or update an index. However, it also introduces
significant added complexity. A separate query marketplace and discovery
registry would be required for finding nodes with the needed
subgraph-index combination.</p>
<h2><a class="header" href="#open-questions-3" id="open-questions-3">Open Questions</a></h2>
<p>Full-text search queries introduce new issues with maintaining query 
result determinism which will become a more potent issue with the 
decentralized network. A fulltext search query and a dataset are not enough 
to determine the output of the query, the index is vital to establish a 
deterministic causal relationship to the output data.  Query verification 
will need to take into account the query, the index, the underlying dataset, 
and the query result.  Can we find a healthy compromise between being 
prescriptive about the indexes and algorithms in order to allow formal 
verification and allowing indexer node operators to experiment with 
algorithms and indexes in order to continue to improve query speed and results? </p>
<p>Since a fulltext search field is purely derivative of other Entity data
the addition or update of an @fulltext directive does not require a full
blockchain resync, rather the index itself just needs to be rebuilt.
There is room for optimization in the future by allowing fulltext search
definition updates without requiring a full subgraph resync.</p>
<h1><a class="header" href="#obsolete-rfcs" id="obsolete-rfcs">Obsolete RFCs</a></h1>
<p>Obsolete RFCs are moved to the <code>rfcs/obsolete</code> directory in the <code>rfcs</code>
repository. They are listed below for reference.</p>
<ul>
<li>No RFCs have been obsoleted yet.</li>
</ul>
<h1><a class="header" href="#rejected-rfcs" id="rejected-rfcs">Rejected RFCs</a></h1>
<p>Rejected RFCs can be found by filtering open and closed pull requests by those
that are labeled with <code>rejected</code>. This list can be <a href="https://github.com/graphprotocol/rfcs/issues?q=label:rfc+label:rejected">found
here</a>.</p>
<h1><a class="header" href="#engineering-plans" id="engineering-plans">Engineering Plans</a></h1>
<h2><a class="header" href="#what-is-an-engineering-plan" id="what-is-an-engineering-plan">What is an Engineering Plan?</a></h2>
<p>Engineering Plans are plans to turn an <a href="engineering-plans/../rfcs/index.html">RFC</a> into an
implementation in the core Graph Protocol tools like Graph Node, Graph CLI and
Graph TS. Every substantial development effort that follows an RFC is planned in
the form of an Engineering Plan.</p>
<h2><a class="header" href="#engineering-plan-process" id="engineering-plan-process">Engineering Plan process</a></h2>
<h3><a class="header" href="#1-create-a-new-engineering-plan" id="1-create-a-new-engineering-plan">1. Create a new Engineering Plan</a></h3>
<p>Like RFCs, Engineering Plans are numbered, starting at <code>0001</code>. To create a new
plan, create a new branch of the <code>rfcs</code> repository. Check the existing plans to
identify the next number to use. Then, copy the <a href="https://github.com/graphprotocol/rfcs/blob/master/engineering-plans/0000-template.md">Engineering Plan
template</a>
to a new file in the <code>engineering-plans/</code> directory. For example:</p>
<pre><code class="language-sh">cp engineering-plans/0000-template.md engineering-plans/0015-fulltext-search.md
</code></pre>
<p>Write the Engineering Plan, commit it to the branch and open a <a href="https://github.com/graphprotocol/rfcs/pulls">pull
request</a> in the <code>rfcs</code> repository.</p>
<p>In addition to the Engineering Plan itself, the pull request must include the
following changes:</p>
<ul>
<li>a link to the Engineering Plan on the <a href="engineering-plans/./approved.html">Approved Engineering Plans</a> page, and</li>
<li>a link to the Engineering Plan under <code>Approved Engineering Plans</code> in <code>SUMMARY.md</code>.</li>
</ul>
<h3><a class="header" href="#2-engineering-plan-review" id="2-engineering-plan-review">2. Engineering Plan review</a></h3>
<p>After an Engineering Plan has been submitted through a pull request, it is being
reviewed. At the time of writing, every Engineering Plan needs to be approved by</p>
<ul>
<li>the Tech Lead, and</li>
<li>at least one member of the core development team.</li>
</ul>
<h3><a class="header" href="#3-engineering-plan-approval" id="3-engineering-plan-approval">3. Engineering Plan approval</a></h3>
<p>Once an Engineering Plan is approved, the Engineering Plan meta data (see the
<a href="https://github.com/graphprotocol/rfcs/blob/master/engineering-plans/0000-template.md">template</a>)
is updated and the pull request is merged by the original author or a Graph
Protocol team member.</p>
<h1><a class="header" href="#approved-engineering-plans" id="approved-engineering-plans">Approved Engineering Plans</a></h1>
<ul>
<li><a href="engineering-plans/./0001-graphql-query-prefetching.html">PLAN-0001: GraphQL Query Prefetching</a></li>
<li><a href="engineering-plans/./0002-ethereum-tracing-cache.html">PLAN-0002: Ethereum Tracing Cache</a></li>
<li><a href="engineering-plans/./0003-remove-jsonb-storage.html">PLAN-0003: Remove JSONB Storage</a></li>
</ul>
<h1><a class="header" href="#plan-0001-graphql-query-prefetching" id="plan-0001-graphql-query-prefetching">PLAN-0001: GraphQL Query Prefetching</a></h1>
<dl>
  <dt>Author</dt>
  <dd>David Lutterkort</dd>
<dt>Implements</dt>
<dd>No RFC - no user visible changes</dd>
<dt>Engineering Plan pull request</dt>
<dd><a href="engineering-plans/<https://github.com/graphprotocol/rfcs/pull/2>">https://github.com/graphprotocol/rfcs/pull/2</a></dd>
<dt>Date of submission</dt>
<dd>2019-11-27</dd>
<dt>Date of approval</dt>
<dd>2019-12-10</dd>
<dt>Approved by</dt>
  <dd>Jannis Pohlmann, Leo Yvens</dd>
</dl>
<p>This is not really a plan as it was written and discussed before we adopted
the RFC process, but contains important implementation detail of how we
process GraphQL queries.</p>
<h2><a class="header" href="#contents-2" id="contents-2">Contents</a></h2>
<ul>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#implementation-details-for-prefetch-queries">Implementation Details for prefetch queries</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#goal">Goal</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#handling-firstskip">Handling first/skip</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#handling-parentchild-relationships">Handling parent/child relationships</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#type-a">Type A</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#type-b">Type B</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#type-c">Type C</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#type-d">Type D</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#handling-interfaces">Handling interfaces</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#boring-list-of-possible-graphql-models">Boring list of possible GraphQL models</a></li>
<li><a href="engineering-plans/0001-graphql-query-prefetching.html#resources">Resources</a></li>
</ul>
<h2><a class="header" href="#implementation-details-for-prefetch-queries" id="implementation-details-for-prefetch-queries">Implementation Details for prefetch queries</a></h2>
<h3><a class="header" href="#goal" id="goal">Goal</a></h3>
<p>For a GraphQL query of the form</p>
<pre><code class="language-graphql">query {
  parents(filter) {
    id
    children(filter) {
      id
    }
  }
}
</code></pre>
<p>we want to generate only two SQL queries: one to get the parents, and one
to get the children for all those parents. The fact that <code>children</code> is
nested under <code>parents</code> requires that we add a filter to the <code>children</code>
query that restricts children to those that are related to the parents we
fetched in the first query to get the parents. How exactly we filter the
<code>children</code> query depends on how the relationship between parents and
children is modeled in the GraphQL schema, and on whether one (or both) of
the types involved are interfaces.</p>
<p>The rest of this writeup is concerned with how to generate the query for
<code>children</code>, assuming we already retrieved the list of all parents.</p>
<p>The bulk of the implementation of this feature can be found in
<code>graphql/src/store/prefetch.rs</code>, <code>store/postgres/src/jsonb_queries.rs</code>, and
<code>store/postgres/src/relational_queries.rs</code></p>
<h3><a class="header" href="#handling-firstskip" id="handling-firstskip">Handling first/skip</a></h3>
<p>We never get all the <code>children</code> for a parent; instead we always have a
<code>first</code> and <code>skip</code> argument in the children filter. Those arguments need to
be applied to each parent individually by ranking the children for each
parent according to the order defined by the <code>children</code> query. If the same
child matches multiple parents, we need to make sure that it is considered
separately for each parent as it might appear at different ranks for
different parents. In SQL, we use a lateral join,  essentially a for
loop. For children that store the id of their parent in <code>parent_id</code>, we'd
run the following query:</p>
<pre><code class="language-sql">select c.*, p.id
  from unnest({parent_ids}) as p(id)
        cross join lateral
         (select *
            from children c
           where c.parent_id = p.id
             and .. other conditions on c ..
           order by c.{sort_key}
           limit {first}
          offset {skip}) c
 order by c.{sort_key}
</code></pre>
<h3><a class="header" href="#handling-parentchild-relationships" id="handling-parentchild-relationships">Handling parent/child relationships</a></h3>
<p>How we get the children for a set of parents depends on how the
relationship between the two is modeled. The interesting parameters there
are whether parents store a list or a single child, and whether that field
is derived, together with the same for children.</p>
<p>There are a total of 16 combinations of these four boolean variables; four
of them, when both parent and child derive their fields, are not
permissible. It also doesn't matter whether the child derives its parent
field: when the parent field is not derived, we need to use that since that
is the only place that contains the parent -&gt; child relationship. When the
parent field is derived, the child field can not be a derived field.</p>
<p>That leaves us with eight combinations of whether the parent
and child store a list or a scalar value, and whether the parent is
derived. For details on the GraphQL schema for each row in this table, see the
section at the end. The <code>Join cond</code> indicates how we can find the children
for a given parent. The table refers to the four different kinds of join
condition we might need as types A, B, C, and D.</p>
<table><thead><tr><th>Case</th><th>Parent list?</th><th>Parent derived?</th><th>Child list?</th><th>Join cond</th><th>Type</th></tr></thead><tbody>
<tr><td>1</td><td>TRUE</td><td>TRUE</td><td>TRUE</td><td>child.parents  parent.id</td><td>A</td></tr>
<tr><td>2</td><td>FALSE</td><td>TRUE</td><td>TRUE</td><td>child.parents  parent.id</td><td>A</td></tr>
<tr><td>3</td><td>TRUE</td><td>TRUE</td><td>FALSE</td><td>child.parent = parent.id</td><td>B</td></tr>
<tr><td>4</td><td>FALSE</td><td>TRUE</td><td>FALSE</td><td>child.parent = parent.id</td><td>B</td></tr>
<tr><td>5</td><td>TRUE</td><td>FALSE</td><td>TRUE</td><td>child.id  parent.children</td><td>C</td></tr>
<tr><td>6</td><td>TRUE</td><td>FALSE</td><td>FALSE</td><td>child.id  parent.children</td><td>C</td></tr>
<tr><td>7</td><td>FALSE</td><td>FALSE</td><td>TRUE</td><td>child.id = parent.child</td><td>D</td></tr>
<tr><td>8</td><td>FALSE</td><td>FALSE</td><td>FALSE</td><td>child.id = parent.child</td><td>D</td></tr>
</tbody></table>
<p>In addition to how the data about the parent/child relationship is stored,
the multiplicity of the parent/child relationship also influences query
generation: if each parent can have at most a single child, queries can be
much simpler than if we have to account for multiple children per parent,
which requires paginating them. We also need to detect cases where the
mappings created multiple children per parent. We do this by adding a
clause <code>limit {parent_ids.len} + 1</code> to the query, so that if there is one
parent with multiple children, we will select it, but still protect
ourselves against mappings that produce catastrophically bad data with huge
numbers of children per parent. The GraphQL execution logic will detect
that there is a parent with multiple children, and generate an error.</p>
<p>When we query children, we already have a list of all parents from running
a previous query. To find the children, we need to have the id of the
parent that child is related to, and, when the parent stores the ids of its
children directly (types C and D) the child ids for each parent id.</p>
<p>The following queries all produce a relation that has the same columns as
the table holding children, plus a column holding the id of the parent that
the child belongs to.</p>
<h4><a class="header" href="#type-a" id="type-a">Type A</a></h4>
<p>Use when parent is derived and child stores a list of parents</p>
<p>Data needed to generate:</p>
<ul>
<li>children: name of child table</li>
<li>parent_ids: list of parent ids</li>
<li>parent_field: name of parents field (array) in child table</li>
<li>single: boolean to indicate whether a parent has at most one child or
not</li>
</ul>
<p>The implementation uses an <code>EntityLink::Direct</code> for joins of this type.</p>
<h5><a class="header" href="#multiple-children-per-parent" id="multiple-children-per-parent">Multiple children per parent</a></h5>
<pre><code class="language-sql">select c.*, p.id as parent_id
  from unnest({parent_ids}) as p(id)
       cross join lateral
       (select *
          from children c
         where p.id = any(c.{parent_field})
           and .. other conditions on c ..
         order by c.{sort_key}
         limit {first} offset {skip}) c
 order by c.{sort_key}
</code></pre>
<h5><a class="header" href="#single-child-per-parent" id="single-child-per-parent">Single child per parent</a></h5>
<pre><code class="language-sql">select c.*, p.id as parent_id
  from unnest({parent_ids}) as p(id),
       children c
 where c.{parent_field} @&gt; array[p.id]
   and .. other conditions on c ..
 limit {parent_ids.len} + 1
</code></pre>
<h4><a class="header" href="#type-b" id="type-b">Type B</a></h4>
<p>Use when parent is derived and child stores a single parent</p>
<p>Data needed to generate:</p>
<ul>
<li>children: name of child table</li>
<li>parent_ids: list of parent ids</li>
<li>parent_field: name of parent field (scalar) in child table</li>
<li>single: boolean to indicate whether a parent has at most one child or
not</li>
</ul>
<p>The implementation uses an <code>EntityLink::Direct</code> for joins of this type.</p>
<h5><a class="header" href="#multiple-children-per-parent-1" id="multiple-children-per-parent-1">Multiple children per parent</a></h5>
<pre><code class="language-sql">select c.*, p.id as parent_id
  from unnest({parent_ids}) as p(id)
       cross join lateral
       (select *
          from children c
         where p.id = c.{parent_field}
           and .. other conditions on c ..
         order by c.{sort_key}
         limit {first} offset {skip}) c
 order by c.{sort_key}
</code></pre>
<h5><a class="header" href="#single-child-per-parent-1" id="single-child-per-parent-1">Single child per parent</a></h5>
<pre><code class="language-sql">select c.*, c.{parent_field} as parent_id
  from children c
 where c.{parent_field} = any({parent_ids})
   and .. other conditions on c ..
 limit {parent_ids.len} + 1
</code></pre>
<p>Alternatively, this is worth a try, too:</p>
<pre><code class="language-sql">select c.*, c.{parent_field} as parent_id
  from unnest({parent_ids}) as p(id), children c
 where c.{parent_field} = p.id
   and .. other conditions on c ..
 limit {parent_ids.len} + 1
</code></pre>
<h4><a class="header" href="#type-c" id="type-c">Type C</a></h4>
<p>Use when the parent stores a list of its children.</p>
<p>Data needed to generate:</p>
<ul>
<li>children: name of child table</li>
<li>parent_ids: list of parent ids</li>
<li>child_id_matrix: array of arrays where <code>child_id_matrix[i]</code> is an array
containing the ids of the children for <code>parent_id[i]</code></li>
</ul>
<p>The implementation uses a <code>EntityLink::Parent</code> for joins of this type.</p>
<h5><a class="header" href="#multiple-children-per-parent-2" id="multiple-children-per-parent-2">Multiple children per parent</a></h5>
<pre><code class="language-sql">select c.*, p.id as parent_id
  from rows from (unnest({parent_ids}), reduce_dim({child_id_matrix}))
              as p(id, child_ids)
       cross join lateral
       (select *
          from children c
         where c.id = any(p.child_ids)
           and .. other conditions on c ..
         order by c.{sort_key}
         limit {first} offset {skip}) c
 order by c.{sort_key}
</code></pre>
<p>Note that <code>reduce_dim</code> is a custom function that is not part of <a href="https://en.wikipedia.org/wiki/SQL:2016">ANSI
SQL:2016</a> but is needed as there is
no standard way to decompose a matrix into a table where each row contains
one row of the matrix. The <code>ROWS FROM</code> construct is also not part of ANSI
SQL.</p>
<h5><a class="header" href="#single-child-per-parent-2" id="single-child-per-parent-2">Single child per parent</a></h5>
<p>Not possible with relations of this type</p>
<h4><a class="header" href="#type-d" id="type-d">Type D</a></h4>
<p>Use when parent is not a list and not derived</p>
<p>Data needed to generate:</p>
<ul>
<li>children: name of child table</li>
<li>parent_ids: list of parent ids</li>
<li>child_ids: list of the id of the child for each parent such that
<code>child_ids[i]</code> is the id of the child for <code>parent_id[i]</code></li>
</ul>
<p>The implementation uses a <code>EntityLink::Parent</code> for joins of this type.</p>
<h5><a class="header" href="#multiple-children-per-parent-3" id="multiple-children-per-parent-3">Multiple children per parent</a></h5>
<p>Not possible with relations of this type</p>
<h5><a class="header" href="#single-child-per-parent-3" id="single-child-per-parent-3">Single child per parent</a></h5>
<pre><code class="language-sql">select c.*, p.id as parent_id
  from rows from (unnest({parent_ids}), unnest({child_ids})) as p(id, child_id),
       children c
 where c.id = p.child_id
   and .. other conditions on c ..
</code></pre>
<p>The <code>ROWS FROM</code> construct is not part of ANSI SQL.</p>
<h3><a class="header" href="#handling-interfaces" id="handling-interfaces">Handling interfaces</a></h3>
<p>If the GraphQL type of the children is an interface, we need to take
special care to form correct queries. Whether the parents are
implementations of an interface or not does not matter, as we will have a
full list of parents already loaded into memory when we build the query for
the children. Whether the GraphQL type of the parents is an interface may
influence from which parent attribute we get child ids for queries of type
C and D.</p>
<p>When the GraphQL type of the children is an interface, we resolve the
interface type into the concrete types implementing it, produce a query for
each concrete child type and combine those queries via <code>union all</code>.</p>
<p>Since implementations of the same interface will generally differ in the
schema they use, we can not form a <code>union all</code> of all the data in the
tables for these concrete types, but have to first query only attributes
that we know will be common to all entities implementing the interface,
most notably the <code>vid</code> (a unique identifier that identifies the precise
version of an entity), and then later fill in the details of each entity by
converting it directly to JSON. A second reason to pass entities as JSON
from the database is that it is impossible with Diesel to execute queries
where the number and types of the columns of the result are not known at
compile time.</p>
<p>We need to to be careful though to not convert to JSONB too early, as that
is slow when done for large numbers of rows. Deferring conversion is
responsible for some of the complexity in these queries.</p>
<p>In the following, we only go through the queries for relational storage;
for JSONB storage, there are similar considerations, though they are
somewhat simpler as the <code>union all</code> in the below queries turns into
an <code>entity = any(..)</code> clause with JSONB storage, and because we do not need
to convert to JSONB data.</p>
<p>That means that when we deal with children that are an interface, we will
first select only the following columns from each concrete child type
(where exactly they come from depends on how the parent/child relationship
is modeled)</p>
<pre><code class="language-sql">select '{__typename}' as entity, c.vid, c.id, c.{sort_key}, p.id as parent_id
</code></pre>
<p>and then use that data to fill in the complete details of each concrete
entity. The query <code>type_query(children)</code> is the query from the previous
section according to the concrete type of <code>children</code>, but without the
<code>select</code>, <code>limit</code>, <code>offset</code> or <code>order by</code> clauses. The overall structure of
this query then is</p>
<pre><code class="language-sql">with matches as (
    select '{children.object}' as entity, c.vid, c.id,
           c.{sort_key}, p.id as parent_id
      from .. type_query(children) ..
     union all
       .. range over all child types ..
     order by {sort_key}
     limit {first} offset {skip})
select m.*, to_jsonb(c.*) as data
  from matches m, {children.table} c
 where c.vid = m.vid and m.entity = '{children.object}'
 union all
       .. range over all child tables ..
 order by {sort_key}
</code></pre>
<p>The list <code>all_parent_ids</code> must contain the ids of all the parents for which
we want to find children.</p>
<p>We have one <code>children</code> object for each concrete GraphQL type that we need
to query, where <code>children.table</code> is the name of the database table in which
these entities are stored, and <code>children.object</code> is the GraphQL typename
for these children.</p>
<p>The code uses an <code>EntityCollection::Window</code> containing multiple
<code>EntityWindow</code> instances to represent the most general form of querying for
the children of a set of parents, the query given above.</p>
<p>When there is only one window, we can simplify the above query. The
simplification basically inlines the <code>matches</code> CTE. That is important as
CTE's in Postgres before Postgres 12 are optimization fences, even when
they are only used once. We therefore reduce the two queries that Postgres
executes above to one for the fairly common case that the children are not
an interface. For each type of parent/child relationship, the resulting
query is essentially the same as the one given in the section
<code>Handling parent/child relationships</code>, except that the <code>select</code> clause is
changed to <code>select '{window.child_type}' as entity, to_jsonb(c.*) as data</code>:</p>
<pre><code class="language-sql">select '..' as entity, to_jsonb(e.*) as data, p.id as parent_id
  from {expand_parents}
       cross join lateral
       (select *
          from children c
         where {linked_children}
           and .. other conditions on c ..
         order by c.{sort_key}
         limit {first} offset {skip}) c
 order by c.{sort_key}
</code></pre>
<p>Toplevel queries, i.e., queries where we have no parents, and therefore do
not restrict the children we return by parent ids are represented in the
code by an <code>EntityCollection::All</code>. If the GraphQL type of the children is
an interface with multiple implementers, we can simplify the query by
avoiding ranking and just using an ordinary <code>order by</code> clause:</p>
<pre><code class="language-sql">with matches as (
  -- Get uniform info for all matching children
  select '{entity_type}' as entity, id, vid, {sort_key}
    from {entity_table} c
   where {query_filter}
   union all
     ... range over all entity types
   order by {sort_key} offset {query.skip} limit {query.first})
-- Get the full entity for each match
select m.entity, to_jsonb(c.*) as data, c.id, c.{sort_key}
  from matches m, {entity_table} c
 where c.vid = m.vid and m.entity = '{entity_type}'
 union all
       ... range over all entity types
 -- Make sure we return the children for each parent in the correct order
     order by c.{sort_key}, c.id
</code></pre>
<p>And finally, for the very common case of a toplevel GraphQL query for a
concrete type, not an interface, we can further simplify this, again by
essentially inlining the <code>matches</code> CTE to:</p>
<pre><code class="language-sql">select '{entity_type}' as entity, to_jsonb(c.*) as data
  from {entity_table} c
 where query.filter()
 order by {query.order} offset {query.skip} limit {query.first}
</code></pre>
<h2><a class="header" href="#boring-list-of-possible-graphql-models" id="boring-list-of-possible-graphql-models">Boring list of possible GraphQL models</a></h2>
<p>These are the eight ways in which a parent/child relationship can be
modeled. For brevity, I left the <code>id</code> attribute on each parent and child
type out.</p>
<p>This list assumes that parent and child types are concrete types, i.e.,
that any interfaces involved in this query have already been reolved into
their implementations and we are dealing with one pair of concrete
parent/child types.</p>
<pre><code class="language-graphql"># Case 1
type Parent {
  children: [Child] @derived
}

type Child {
  parents: [Parent]
}

# Case 2
type Parent {
  child: Child @derived
}

type Child {
  parents: [Parent]
}

# Case 3
type Parent {
  children: [Child] @derived
}

type Child {
  parent: Parent
}

# Case 4
type Parent {
  child: Child @derived
}

type Child {
  parent: Parent
}

# Case 5
type Parent {
  children: [Child]
}

type Child {
  # doesn't matter
}

# Case 6
type Parent {
  children: [Child]
}

type Child {
  # doesn't matter
}

# Case 7
type Parent {
  child: Child
}

type Child {
  # doesn't matter
}

# Case 8
type Parent {
  child: Child
}

type Child {
  # doesn't matter
}
</code></pre>
<h2><a class="header" href="#resources" id="resources">Resources</a></h2>
<ul>
<li><a href="https://www.postgresql.org/docs/12/index.html">PostgreSQL Manual</a></li>
<li><a href="https://jakewheat.github.io/sql-overview/sql-2016-foundation-grammar.html">Browsable SQL Grammar</a></li>
<li><a href="https://en.wikipedia.org/wiki/SQL:2016">Wikipedia entry on ANSI SQL:2016</a> The actual standard is not freely available</li>
</ul>
<h1><a class="header" href="#plan-0002-ethereum-tracing-cache" id="plan-0002-ethereum-tracing-cache">PLAN-0002: Ethereum Tracing Cache</a></h1>
<dl>
  <dt>Author</dt>
  <dd>Zachary Burns</dd>
<dt>Implements</dt>
  <dd><a href="engineering-plans/../rfcs/0002-ethereum-tracing-cache.html">RFC-0002 Ethereum Tracing Cache</a></dd>
<dt>Engineering Plan pull request</dt>
  <dd><a href="https://github.com/graphprotocol/rfcs/pull/9">https://github.com/graphprotocol/rfcs/pull/9</a></dd>
<dt>Date of submission</dt>
  <dd>2019-12-20</dd>
<dt>Date of approval</dt>
  <dd>2020-01-07</dd>
<dt>Approved by</dt>
  <dd>Jannis Pohlmann, Leo Yvens</dd>
</dl>
<h2><a class="header" href="#summary-4" id="summary-4">Summary</a></h2>
<p>Implements RFC-0002: Ethereum Tracing Cache</p>
<h2><a class="header" href="#implementation" id="implementation">Implementation</a></h2>
<p>These changes happen within or near <code>ethereum_adapter.rs</code>, <code>store.rs</code> and <code>db_schema.rs</code>.</p>
<h3><a class="header" href="#limitations" id="limitations">Limitations</a></h3>
<p>The problem of reorg turns out to be a particularly tricky one for the cache, mostly due to ranges of blocks being requested rather than individual hashes. To sidestep this problem, only blocks that are older than the reorg threshold will be eligible for caching.</p>
<p>Additionally, there are some subgraphs which may require traces from all or a substantial number of blocks and don't make effective use of filtering. In particular, subgraphs which specify a call handler without a contract address fall into this category. In order to prevent the cache from bloating, any use of Ethereum traces which does not filter on a contract address will bypass the cache.</p>
<h3><a class="header" href="#ethereumtracecache" id="ethereumtracecache">EthereumTraceCache</a></h3>
<p>The implementation introduces the following trait, which is implemented primarily by <code>Store</code>.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>use std::ops::RangeInclusive;
struct TracesInRange {
    range: RangeInclusive&lt;u64&gt;,
    traces: Vec&lt;Trace&gt;,
}

pub trait EthereumTraceCache: Send + Sync + 'static {
    /// Attempts to retrieve traces from the cache. Returns ranges which were retrieved.
    /// The results may not cover the entire range of blocks. It is up to the caller to decide
    /// what to do with ranges of blocks that are not cached.
    fn traces_for_blocks(contract_address: Option&lt;H160&gt;, blocks: RangeInclusive&lt;u64&gt;
        ) -&gt; Box&lt;dyn Future&lt;Output=Result&lt;Vec&lt;TracesInRange&gt;, Error&gt;&gt;&gt;;
    fn add(contract_address: Option&lt;H160&gt;, traces: Vec&lt;TracesInRange&gt;);
}
<span class="boring">}
</span></code></pre></pre>
<h4><a class="header" href="#block-schema" id="block-schema">Block schema</a></h4>
<p>Each cached block will exist as its own row in the database in an <code>eth_traces_cache</code> table.</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>eth_traces_cache(id) {
  id -&gt; Integer,
  network -&gt; Text,
  block_number: Integer,
  contract_address: Bytea,
  traces -&gt; Jsonb,
}
<span class="boring">}
</span></code></pre></pre>
<p>A multi-column index will be added on network, block_number, and contract_address.</p>
<p>It can be noted that in the <code>eth_traces_cache</code> table, there is a very low cardinality for the value of the network row. It is inefficient for example to store the string <code>mainnet</code> millions of times and consider this value when querying. A data oriented approach would be to partition these tables on the value of the network. It is expected that hash partitioning available in Postgres 11 would be useful here, but the necessary dependencies won't be ready in time for this RFC. This may be revisited in the future.</p>
<h4><a class="header" href="#valid-cache-range" id="valid-cache-range">Valid Cache Range</a></h4>
<p>Because the absence of trace data for a block is a valid cache result, the database must maintain a data structure indicating which ranges of the cache are valid in an <code>eth_traces_meta</code> table. This table also enables eventually implementing cleaning out old data.</p>
<p>This is the schema for that structure:</p>
<pre><pre class="playpen"><code class="language-rust">
<span class="boring">#![allow(unused_variables)]
</span><span class="boring">fn main() {
</span>id -&gt; Integer,
network -&gt; Text,
start_block -&gt; Integer,
end_block -&gt; Integer,
contract_address -&gt; Nullable&lt;Bytea&gt;,
accessed_at -&gt; Date,
<span class="boring">}
</span></code></pre></pre>
<p>When inserting data into the cache, removing data from the cache, or reading the cache, a serialized transaction must be used to preserve atomicity between the valid cache range structure and the cached blocks. Care must be taken to not rely on any data read outside of the serialized transaction, and for the extent of the serialized transaction to not span any async contexts that rely on any <code>Future</code> outside of the database itself. The definition of the <code>EthereumTraceCache</code> trait is designed to uphold these guarantees.</p>
<p>In order to preserve space in the database, whenever the valid cache range is added it will be added such that adjacent and overlapping ranges are merged into it.</p>
<h3><a class="header" href="#cache-usage" id="cache-usage">Cache usage</a></h3>
<p>The primary user of the cache is <code>EtheriumAdapter&lt;T&gt;</code> in the <code>traces</code> function. </p>
<p>The correct algorithm for retrieving traces from the cache is surprisingly nuanced. The complication arises from the interaction between multiple subgraphs which may require a subset of overlapping contract addresses. The rate at which indexing proceeds of these subgraphs can cause different ranges of the cache to be valid for a contract address in a single query.</p>
<p>We want to minimize the cost of external requests for trace data. It is likely that it is better to...</p>
<ul>
<li>Make fewer requests</li>
<li>Not ask for trace data that is already cached</li>
<li>Ask for trace data for multiple contract addresses within the same block when possible.</li>
</ul>
<p>There is one flow of data which upholds these invariants. In doing so it makes a tradeoff of increasing latency for the execution of a specific subgraph, but increases throughput of the whole system.</p>
<p>Within this graph:</p>
<ul>
<li>Edges which are labelled refer to some subset of the output data.</li>
<li>Edges which are not labelled refer to the entire set of the output data.</li>
<li>Each node executes once for each contiguous range of blocks. That is, it merges all incoming data before executing, and executes the minimum possible times.</li>
<li>The example given is just for 2 addresses. The actual code must work on sets of addresses.</li>
</ul>
<pre class="mermaid">graph LR;
   A[Block Range for Contract A & B]
   A --> |Above Reorg Threshold| E
   D[Get Cache A]
   A --> |Below Reorg Threshold A| D
   A --> |Below Reorg Threshold B| H
   E[Ethereum A & B]
   F[Ethereum A]
   G[Ethereum B]
   H[Get Cache B]
   D --> |Found| M
   H --> |Found| M
   M[Result]
   D --> |Missing| N
   H --> |Missing| N
   N[Overlap]
   N --> |A & B| E
   N --> |A| F
   N --> |B| G
   E --> M
   K[Set Cache A]
   L[Set Cache B]
   E --> |B Below Reorg Threshold| L
   E --> |A Below Reorg Threshold| K
   F --> K
   G --> L
   F --> M
   G --> M
</pre>
<p>This construction is designed to make the fewest number of the most efficient calls possible. It is not as complicated as it looks. The actual construction can be expressed as sequential steps with a set of filters preceding each step.</p>
<h3><a class="header" href="#useful-dependencies" id="useful-dependencies">Useful dependencies</a></h3>
<p>The feature deals a lot with ranges and sets. Operations like sum, subtract, merge, and find overlapping are used frequently. <a href="https://crates.io/crates/nested_intervals">nested_intervals</a> is a crate which provides some of these operations.</p>
<h2><a class="header" href="#tests" id="tests">Tests</a></h2>
<h3><a class="header" href="#benchmark" id="benchmark">Benchmark</a></h3>
<p>A temporary benchmark will be added for indexing a simple subgraph which uses call handlers. The benchmark will be run in these scenarios:</p>
<ul>
<li>Sync before changes</li>
<li>Re-sync before changes </li>
<li>Sync after changes</li>
<li>Re-sync after changes</li>
</ul>
<h3><a class="header" href="#ranges" id="ranges">Ranges</a></h3>
<p>Due to the complexity of the resource minimizing data workflow, it will be useful to have mocks for the cache and database which record their calls, and check that expected calls are made for tricky data sets.</p>
<h3><a class="header" href="#database" id="database">Database</a></h3>
<p>A real database integration test will be added to test the add/remove from cache implementation to verify that it correctly merges blocks, handles concurrency issues, etc.</p>
<h2><a class="header" href="#migration" id="migration">Migration</a></h2>
<p>None</p>
<h2><a class="header" href="#documentation" id="documentation">Documentation</a></h2>
<p>None, aside from code comments</p>
<h2><a class="header" href="#implementation-plan" id="implementation-plan">Implementation Plan:</a></h2>
<p>These estimates inflated to account for the author's lack of experience with Postgres, Ethereum, Futures0.1, and The Graph in general.</p>
<ul>
<li>(1) Create benchmarks</li>
<li>Postgres Cache
<ul>
<li>(0.5) Block Cache</li>
<li>(0.5) Trace Serialization/Deserialization</li>
<li>(1.0) Ranges Cache</li>
<li>(0.5) Concurrency/Transactions</li>
<li>(0.5) Tests against Postgres</li>
</ul>
</li>
<li>Data Flow
<ul>
<li>(3) Implementation</li>
<li>(1) Unit tests</li>
</ul>
</li>
<li>(0.5) Run Benchmarks</li>
</ul>
<p>Total: 8</p>
<h1><a class="header" href="#plan-0003-remove-jsonb-storage" id="plan-0003-remove-jsonb-storage">PLAN-0003: Remove JSONB Storage</a></h1>
<dl>
  <dt>Author</dt>
  <dd>David Lutterkort</dd>
<dt>Implements</dt>
  <dd>No RFC - no user visible changes</dd>
<dt>Engineering Plan pull request</dt>
  <dd><a href="https://github.com/graphprotocol/rfcs/pull/7">https://github.com/graphprotocol/rfcs/pull/7</a></dd>
<dt>Date of submission</dt>
  <dd>2019-12-18</dd>
<dt>Date of approval</dt>
  <dd>2019-12-20</dd>
<dt>Approved by</dt>
  <dd>Jess Ngo, Jannis Pohlmann</dd>
</dl>
<h2><a class="header" href="#summary-5" id="summary-5">Summary</a></h2>
<p>Remove JSONB storage from <code>graph-node</code>. That means that we want to remove
the old storage scheme, and only use relational storage going
forward. At a high level, removal has to touch the following areas:</p>
<ul>
<li>user subgraphs in the hosted service</li>
<li>user subgraphs in self-hosted <code>graph-node</code> instances</li>
<li>subgraph metadata in <code>subgraphs.entities</code> (see <a href="https://github.com/graphprotocol/graph-node/issues/1394">this issue</a>)</li>
<li>the <code>graph-node</code> code base</li>
</ul>
<p>Because it touches so many areas and different things, JSONB storage
removal will need to happen in several steps, the last being actual removal
of JSONB code. The first three steps above are independent of each other
and can be done in parallel.</p>
<h2><a class="header" href="#implementation-1" id="implementation-1">Implementation</a></h2>
<h3><a class="header" href="#user-subgraphs-in-the-hosted-service" id="user-subgraphs-in-the-hosted-service">User Subgraphs in the Hosted Service</a></h3>
<p>We will need to communicate to users that they need to update their
subgraphs if they still use JSONB storage. Currently, there are ~ 580
subgraphs
(<a href="https://gist.github.com/lutter/2e7a7716b70b4144fe0b6a5f1c9066bc">list</a>)
belonging to 220 different organizations using JSONB storage. It is quite
likely that the vast majority of them is not needed anymore and simply left
over from somebody trying something out.</p>
<p>We should contact users and tell them that we will delete their subgraph
after a certain date (say 2020-02-01) <em>unless</em> they deploy a new version of
the subgraph (with an explanation why etc. of course) Redeploying their
subgraph is all that is needed for those updates.</p>
<h3><a class="header" href="#self-hosted-user-subgraphs" id="self-hosted-user-subgraphs">Self-hosted User Subgraphs</a></h3>
<p>We will need to tell users that the 'old' JSONB storage is deprecated and
support for it will be removed as of some target date, and that they need
to redeploy their subgraph.</p>
<p>Users will need some documentation/tooling to help them understand</p>
<ul>
<li>which of their deployed subgraphs still use JSONB storage</li>
<li>how to remove old subgraphs</li>
<li>how to remove old deployments</li>
</ul>
<h3><a class="header" href="#subgraph-metadata-in-subgraphsentities" id="subgraph-metadata-in-subgraphsentities">Subgraph Metadata in <code>subgraphs.entities</code></a></h3>
<p>We can treat the <code>subgraphs</code> schema like a normal subgraph, with the
exception that some entities must not be versioned. For that, we will need
to adopt code that makes it possible to write entities to the store without
recording their version (or, more generally, so that there will only be one
version of the entity, tagged with a block range <code>[0,)</code>)</p>
<p>We will manually create the DDL for the <code>subgraphs.graphql</code> schema and run
that as part of a database migration. In that migration, we will also copy
the existing metadata from <code>subgraphs.entities</code> and
<code>subgraphs.entity_history</code> into their new tables.</p>
<h3><a class="header" href="#the-code-base" id="the-code-base">The Code Base</a></h3>
<p>Delete all code handling JSONB storage. This will mostly affect
<code>entities.rs</code> and <code>jsonb_queries.rs</code> in <code>graph-store-postgres</code>, but there
are also smaller things like that we do not need the annotations on
<code>Entity</code> to serialize them to the JSON format that JSONB uses.</p>
<h2><a class="header" href="#tests-1" id="tests-1">Tests</a></h2>
<p>Most of the code-level changes are covered by the existing test suite. The
major exception is that the migration of subgraph metadata needs to be
tested and checked manually, using a recent dump of the production
database.</p>
<h2><a class="header" href="#migration-1" id="migration-1">Migration</a></h2>
<p>See above on migrating data in the <code>subgraphs</code> schema.</p>
<h2><a class="header" href="#documentation-1" id="documentation-1">Documentation</a></h2>
<p>No user-facing documentation is needed.</p>
<h2><a class="header" href="#implementation-plan-1" id="implementation-plan-1">Implementation Plan</a></h2>
<p><em>No estimates yet as we should first agree on this general course of
action</em></p>
<ul>
<li>Notify hosted users to update their subgraph or have it deleted by date X</li>
<li>Mark JSONB storage as deprecated and announce when it will be removed</li>
<li>Provide tool to ship with <code>graph-node</code> to delete unused deployments and
unneeded subgraphs</li>
<li>Add affordance to not version entities to relational storage code</li>
<li>Write SQL migrations to create new subgraph metadata schema and copy
existing data</li>
<li>Delete old JSONB code</li>
<li>On start of <code>graph-node</code>, add check for any deployments that still use
JSONB storage and log warning messages telling users to redeploy (once
the JSONB code has been deleted, this data can not be accessed any more)</li>
</ul>
<h2><a class="header" href="#open-questions-4" id="open-questions-4">Open Questions</a></h2>
<p>None</p>
<h1><a class="header" href="#obsolete-engineering-plans" id="obsolete-engineering-plans">Obsolete Engineering Plans</a></h1>
<p>Obsolete Engineering Plans are moved to the <code>engineering-plans/obsolete</code>
directory in the <code>rfcs</code> repository. They are listed below for reference.</p>
<ul>
<li>No Engineering Plans have been obsoleted yet.</li>
</ul>
<h1><a class="header" href="#rejected-engineering-plans" id="rejected-engineering-plans">Rejected Engineering Plans</a></h1>
<p>Rejected Engineering Plans can be found by filtering open and closed pull
requests by those that are labeled with <code>rejected</code>. This list can be <a href="https://github.com/graphprotocol/rfcs/issues?q=label:engineering-plan+label:rejected">found
here</a>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        
        
        
        <script type="text/javascript">
            window.playpen_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
        <script type="text/javascript" src="assets/mermaid.min.js"></script>
        
        <script type="text/javascript" src="assets/mermaid-init.js"></script>
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
